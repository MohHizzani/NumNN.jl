<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Docstrings · NumNN.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">NumNN.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>Docstrings</a></li><li><a class="tocitem" href="../tutorials/">Tutorials</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Docstrings</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Docstrings</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/MohHizzani/NumNN.jl/blob/master/docs/src/docstrings.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Docstrings-1"><a class="docs-heading-anchor" href="#Docstrings-1">Docstrings</a><a class="docs-heading-anchor-permalink" href="#Docstrings-1" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="NumNN.Activation" href="#NumNN.Activation"><code>NumNN.Activation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Activation(actFun)</code></pre><p><strong>Arguments</strong></p><ul><li><code>actFun::Symbol</code> := the activation function of this layer</li></ul><hr/><p><strong>Summary</strong></p><p>mutable struct Activation &lt;: Layer</p><hr/><p><strong>Fields</strong></p><ul><li><p><code>actFun::Symbol</code> := the activation function of this layer</p></li><li><p><code>channels::Integer</code> := is the number of nodes or <code>channels</code> in the layer</p></li><li><p><code>inputS::Tuple</code> := input size of the layer</p></li><li><p><code>outputS::Tuple</code> := output size of the layer</p></li><li><p><code>forwCount::Integer</code> := forward propagation counter</p></li><li><p><code>backCount::Integer</code> := backward propagation counter</p></li><li><p><code>updateCount::Integer</code> := update parameters counter</p></li><li><p><code>nextLayers::Array{Layer,1}</code> := An array of the next <code>layer</code>(s)</p></li><li><p><code>prevLayer::Array{Layer,1}</code> := An array of the previous <code>layer</code>(s) to be added</p></li></ul><hr/><p><strong>Supertype Hierarchy</strong></p><pre><code class="language-none">Activation &lt;: Layer &lt;: An</code></pre><hr/><p><strong>Examples</strong></p><pre><code class="language-julia">X_Input = Input(X_train)
X = FCLayer(10, :noAct)(X_Input)
X = Activation(:relu)(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/TypeDef.jl#L328-L379">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.AddLayer" href="#NumNN.AddLayer"><code>NumNN.AddLayer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AddLayer(; [channels = 0])</code></pre><p>Layer performs and addition of multiple previous layers</p><p><strong>Arguments</strong></p><ul><li><code>channels</code> := (<code>Integer</code>) number of channels/nodes of this array which equals to the same of the previous layer(s)</li></ul><hr/><p><strong>Summary</strong></p><pre><code class="language-none">mutable struct AddLayer &lt;: MILayer</code></pre><p><strong>Fields</strong></p><ul><li><p><code>channels::Integer</code> := is the number of nodes or <code>channels</code> in the layer</p></li><li><p><code>inputS::Tuple</code> := input size of the layer</p></li><li><p><code>outputS::Tuple</code> := output size of the layer</p></li><li><p><code>forwCount::Integer</code> := forward propagation counter</p></li><li><p><code>backCount::Integer</code> := backward propagation counter</p></li><li><p><code>updateCount::Integer</code> := update parameters counter</p></li><li><p><code>nextLayers::Array{Layer,1}</code> := An array of the next <code>layer</code>(s)</p></li><li><p><code>prevLayer::Array{Layer,1}</code> := An array of the previous <code>layer</code>(s) to be added</p></li></ul><hr/><p><strong>Supertype Hierarchy</strong></p><pre><code class="language-none">AddLayer &lt;: MILayer &lt;: Layer &lt;: Any</code></pre><hr/><p><strong>Examples</strong></p><pre><code class="language-julia">XIn1 = Input(X_train)
X1 = FCLayer(10, :relu)(XIn1)
XIn2 = Input(X_train)
X2 = FCLayer(10, :tanh)(XIn2)

Xa = AddLayer()([X1,X2])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/TypeDef.jl#L170-L222">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.AveragePool1D" href="#NumNN.AveragePool1D"><code>NumNN.AveragePool1D</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AveragePool1D(
    f::Integer=2;
    prevLayer=nothing,
    strides::Integer=f,
    padding::Symbol=:valid,
)</code></pre><p><strong>Summary</strong></p><pre><code class="language-none">mutable struct AveragePool1D &lt;: AveragePoolLayer</code></pre><p><strong>Fields</strong></p><pre><code class="language-none">channels    :: Integer
f           :: Integer
s           :: Integer
inputS      :: Tuple
outputS     :: Tuple
padding     :: Symbol
forwCount   :: Integer
backCount   :: Integer
updateCount :: Integer
prevLayer   :: Union{Nothing, Layer}
nextLayers  :: Array{Layer,1}</code></pre><p><strong>Supertype Hierarchy</strong></p><pre><code class="language-none">AveragePool1D &lt;: AveragePoolLayer &lt;: PoolLayer &lt;: PaddableLayer &lt;: Layer &lt;: Any</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/TypeDef.jl#L910-L939">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.AveragePool2D" href="#NumNN.AveragePool2D"><code>NumNN.AveragePool2D</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AveragePool2D(
    f::Tuple{Integer,Integer}=(2,2);
    prevLayer=nothing,
    strides::Tuple{Integer,Integer}=f,
    padding::Symbol=:valid,
)</code></pre><p><strong>Summary</strong></p><pre><code class="language-none">mutable struct AveragePool2D &lt;: AveragePoolLayer</code></pre><p><strong>Fields</strong></p><pre><code class="language-none">channels    :: Integer
f           :: Tuple{Integer,Integer}
s           :: Tuple{Integer,Integer}
inputS      :: Tuple
outputS     :: Tuple
padding     :: Symbol
forwCount   :: Integer
backCount   :: Integer
updateCount :: Integer
prevLayer   :: Union{Nothing, Layer}
nextLayers  :: Array{Layer,1}</code></pre><p><strong>Supertype Hierarchy</strong></p><pre><code class="language-none">AveragePool2D &lt;: AveragePoolLayer &lt;: PoolLayer &lt;: PaddableLayer &lt;: Layer &lt;: Any</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/TypeDef.jl#L815-L845">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.AveragePool3D" href="#NumNN.AveragePool3D"><code>NumNN.AveragePool3D</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AveragePool3D(
    f::Tuple{Integer,Integer,Integer}=(2,2,2);
    prevLayer=nothing,
    strides::Tuple{Integer,Integer,Integer}=f,
    padding::Symbol=:valid,
)</code></pre><p><strong>Summary</strong></p><pre><code class="language-none">mutable struct AveragePool3D &lt;: AveragePoolLayer</code></pre><p><strong>Fields</strong></p><pre><code class="language-none">channels    :: Integer
f           :: Tuple{Integer,Integer,Integer}
s           :: Tuple{Integer,Integer,Integer}
inputS      :: Tuple
outputS     :: Tuple
padding     :: Symbol
forwCount   :: Integer
backCount   :: Integer
updateCount :: Integer
prevLayer   :: Union{Nothing, Layer}
nextLayers  :: Array{Layer,1}</code></pre><p><strong>Supertype Hierarchy</strong></p><pre><code class="language-none">AveragePool3D &lt;: AveragePoolLayer &lt;: PoolLayer &lt;: PaddableLayer &lt;: Layer &lt;: Any</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/TypeDef.jl#L1004-L1034">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.AveragePoolLayer" href="#NumNN.AveragePoolLayer"><code>NumNN.AveragePoolLayer</code></a> — <span class="docstring-category">Type</span></header><section><div><p><strong>Summary</strong></p><pre><code class="language-none">abstract type AveragePoolLayer &lt;: PoolLayer</code></pre><p><strong>Subtypes</strong></p><pre><code class="language-none">AveragePool1D
AveragePool2D
AveragePool3D</code></pre><p><strong>Supertype Hierarchy</strong></p><pre><code class="language-none">AveragePoolLayer &lt;: PoolLayer &lt;: PaddableLayer &lt;: Layer &lt;: Any</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/TypeDef.jl#L796-L810">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.BatchNorm" href="#NumNN.BatchNorm"><code>NumNN.BatchNorm</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">BatchNorm(;dim=1, ϵ=1e-10)</code></pre><p>Batch Normalization <code>Layer</code> that is used ot normalize across the dimensions specified by the argument <code>dim</code>.</p><p><strong>Arguments</strong></p><ul><li><p><code>dim::Integer</code> := is the dimension to normalize across</p></li><li><p><code>ϵ::AbstractFloat</code> := is a backup constant that is used to prevent from division on zero when <span>$σ^2$</span> is zero</p></li></ul><hr/><p><strong>Summary</strong></p><pre><code class="language-none">mutable struct BatchNorm &lt;: Layer</code></pre><hr/><p><strong>Fields</strong></p><ul><li><p><code>channels::Integer</code> := is the number of nodes in the layer</p></li><li><p><code>inputS::Tuple{Integer, Integer}</code> := input size of the layer, of the shape (channels of the previous layer, size of mini-batch)</p></li><li><p><code>outputS::Tuple{Integer, Integer}</code> := output size of the layer, of the shape (channels of this layer, size of mini-batch)</p></li><li><p><code>dim::Integer</code> := the dimension to normalize across</p></li><li><p><code>ϵ::AbstractFloat</code> := backup constant to protect from dividing on zero when <span>$σ^2 = 0$</span></p></li><li><p><code>W::Array{T,2} where {T}</code> := the scaling parameters of this layer <code>W * X</code>, same shape of the mean <code>μ</code></p></li><li><p><code>B::Array{T,2} where {T}</code> := the bias of this layer <code>W * X .+ B</code>, same shape of the variance <span>$σ^2$</span></p></li><li><p><code>dW::Array{T,2} where {T}</code> := the derivative of the loss function to the W parameters <span>$\frac{dJ}{dW}$</span></p></li><li><p><code>dB::Array{T,2} where {T}</code> := the derivative of the loss function to the B parameters <span>$\frac{dJ}{dB}$</span></p></li><li><p><code>forwCount::Integer</code> := forward propagation counter</p></li><li><p><code>backCount::Integer</code> := backward propagation counter</p></li><li><p><code>updateCount::Integer</code> := update parameters counter</p></li><li><p><code>prevLayer::L where {L&lt;:Union{Layer,Nothing}}</code> := the previous layer which is</p></li></ul><p>the input of this layer</p><ul><li><code>nextLayers::Array{Layer,1}</code> := An array of the next <code>layer</code>(s)</li></ul><hr/><p><strong>Supertype Hierarchy</strong></p><pre><code class="language-none">BatchNorm &lt;: Layer &lt;: Any</code></pre><hr/><p><strong>Examples</strong></p><pre><code class="language-julia">X_train = rand(14,14,3,32) #input of shape `14×14` with channels of `3` and mini-batch size `32`

X_Input = Input(X_train)
X = Conv2D(10, (3,3))(X_Input)
X = BatchNorm(dim=3) #to normalize across the channels dimension
X = Activation(:relu)</code></pre><pre><code class="language-julia">X_train = rand(128,5,32) #input of shape `128` with channels of `5` and mini-batch size `32`

X_Input = Input(X_train)
X = Conv1D(10, 5)(X_Input)
X = BatchNorm(dim=2) #to normalize across the channels dimension
X = Activation(:relu)
``

</code></pre><p>julia X_train = rand(64<em>64,32) #input of shape `64</em>64<code>and mini-batch size</code>32`</p><p>X<em>Input = Input(X</em>train) X = FCLayer(10, :noAct)(X_Input) X = BatchNorm(dim=1) #to normalize across the features dimension X = Activation(:relu) ````</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/TypeDef.jl#L521-L609">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.ConcatLayer" href="#NumNN.ConcatLayer"><code>NumNN.ConcatLayer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ConcatLayer(; channels = 0)</code></pre><p>Perform concatenation of group of previous <code>Layer</code>s</p><p><strong>Summary</strong></p><pre><code class="language-none">mutable struct ConcatLayer &lt;: MILayer</code></pre><p><strong>Fields</strong></p><pre><code class="language-none">channels    :: Integer
inputS      :: Tuple
outputS     :: Tuple
forwCount   :: Integer
backCount   :: Integer
updateCount :: Integer
nextLayers  :: Array{Layer,1}
prevLayer   :: Array{Layer,1}</code></pre><p><strong>Supertype Hierarchy</strong></p><pre><code class="language-none">ConcatLayer &lt;: MILayer &lt;: Layer &lt;: Any</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/TypeDef.jl#L265-L288">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.Conv1D" href="#NumNN.Conv1D"><code>NumNN.Conv1D</code></a> — <span class="docstring-category">Type</span></header><section><div><p><strong>Summary</strong></p><pre><code class="language-none">mutable struct Conv1D &lt;: ConvLayer</code></pre><p><strong>Fields</strong></p><pre><code class="language-none">channels    :: Integer
f           :: Integer
s           :: Integer
inputS      :: Tuple
outputS     :: Tuple
padding     :: Symbol
W           :: Array{F,3} where F
dW          :: Array{F,3} where F
K           :: Array{F,2} where F
dK          :: Array{F,2} where F
B           :: Array{F,3} where F
dB          :: Array{F,3} where F
actFun      :: Symbol
keepProb    :: AbstractFloat
V           :: Dict{Symbol,Array{F,3} where F}
S           :: Dict{Symbol,Array{F,3} where F}
V̂dk         :: Array{F,2} where F
Ŝdk         :: Array{F,2} where F
forwCount   :: Integer
backCount   :: Integer
updateCount :: Integer
prevLayer   :: Union{Nothing, Layer}
nextLayers  :: Array{Layer,1}</code></pre><p><strong>Supertype Hierarchy</strong></p><pre><code class="language-none">Conv1D &lt;: ConvLayer &lt;: PaddableLayer &lt;: Layer &lt;: Any</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/TypeDef.jl#L188-L222">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.Conv2D" href="#NumNN.Conv2D"><code>NumNN.Conv2D</code></a> — <span class="docstring-category">Type</span></header><section><div><p><strong>Summary</strong></p><pre><code class="language-none">mutable struct Conv2D &lt;: ConvLayer</code></pre><p><strong>Fields</strong></p><pre><code class="language-none">channels    :: Integer
f           :: Tuple{Integer,Integer}
s           :: Tuple{Integer,Integer}
inputS      :: Tuple
outputS     :: Tuple
padding     :: Symbol
W           :: Array{F,4} where F
dW          :: Array{F,4} where F
K           :: Array{F,2} where F
dK          :: Array{F,2} where F
B           :: Array{F,4} where F
dB          :: Array{F,4} where F
actFun      :: Symbol
keepProb    :: AbstractFloat
V           :: Dict{Symbol,Array{F,4} where F}
S           :: Dict{Symbol,Array{F,4} where F}
V̂dk         :: Array{F,2} where F
Ŝdk         :: Array{F,2} where F
forwCount   :: Integer
backCount   :: Integer
updateCount :: Integer
prevLayer   :: Union{Nothing, Layer}
nextLayers  :: Array{Layer,1}</code></pre><p><strong>Supertype Hierarchy</strong></p><pre><code class="language-none">Conv2D &lt;: ConvLayer &lt;: PaddableLayer &lt;: Layer &lt;: Any</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/TypeDef.jl#L43-L77">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.Conv3D" href="#NumNN.Conv3D"><code>NumNN.Conv3D</code></a> — <span class="docstring-category">Type</span></header><section><div><p><strong>Summary</strong></p><pre><code class="language-none">mutable struct Conv3D &lt;: ConvLayer</code></pre><p><strong>Fields</strong></p><pre><code class="language-none">channels    :: Integer
f           :: Tuple{Integer,Integer,Integer}
s           :: Tuple{Integer,Integer,Integer}
inputS      :: Tuple
outputS     :: Tuple
padding     :: Symbol
W           :: Array{F,5} where F
dW          :: Array{F,5} where F
K           :: Array{F,2} where F
dK          :: Array{F,2} where F
B           :: Array{F,5} where F
dB          :: Array{F,5} where F
actFun      :: Symbol
keepProb    :: AbstractFloat
V           :: Dict{Symbol,Array{F,5} where F}
S           :: Dict{Symbol,Array{F,5} where F}
V̂dk        :: Array{F,2} where F
Ŝdk         :: Array{F,2} where F
forwCount   :: Integer
backCount   :: Integer
updateCount :: Integer
prevLayer   :: Union{Nothing, Layer}
nextLayers  :: Array{Layer,1}</code></pre><p><strong>Supertype Hierarchy</strong></p><pre><code class="language-none">Conv3D &lt;: ConvLayer &lt;: PaddableLayer &lt;: Layer &lt;: Any</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/TypeDef.jl#L328-L362">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.ConvLayer" href="#NumNN.ConvLayer"><code>NumNN.ConvLayer</code></a> — <span class="docstring-category">Type</span></header><section><div><p><strong>Summary</strong></p><pre><code class="language-none">abstract type ConvLayer &lt;: PaddableLayer</code></pre><p>Abstract Type to hold all ConvLayer</p><p><strong>Subtypes</strong></p><pre><code class="language-none">Conv1D
Conv2D
Conv3D</code></pre><p><strong>Supertype Hierarchy</strong></p><pre><code class="language-none">ConvLayer &lt;: PaddableLayer &lt;: Layer &lt;: Any</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/TypeDef.jl#L20-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.FCLayer" href="#NumNN.FCLayer"><code>NumNN.FCLayer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">FCLayer(channels=0, actFun=:noAct, [layerInput = nothing; keepProb = 1.0])</code></pre><p>Fully-connected layer (equivalent to Dense in TensorFlow etc.)</p><p><strong>Arguments</strong></p><ul><li><p><code>channels</code> := (<code>Integer</code>) is the number of nodes in the layer</p></li><li><p><code>actFun</code> := (<code>Symbol</code>) is the activation function of this layer</p></li><li><p><code>layerInput</code> := (<code>Layer</code> or <code>Array</code>) the input of this array (optional don&#39;t need to assign it)</p></li><li><p><code>keepProb</code> := (<code>AbstractFloat</code>) the keep probability (1 - prob of the dropout rate)</p></li></ul><hr/><p><strong>Summary</strong></p><pre><code class="language-none">mutable struct FCLayer &lt;: Layer</code></pre><hr/><p><strong>Fields</strong></p><ul><li><p><code>channels::Integer</code> := is the number of nodes in the layer</p></li><li><p><code>actFun::Symbol</code> := the activation function of this layer</p></li><li><p><code>inputS::Tuple{Integer, Integer}</code> := input size of the layer, of the shape (channels of the previous layer, size of mini-batch)</p></li><li><p><code>outputS::Tuple{Integer, Integer}</code> := output size of the layer, of the shape (channels of this layer, size of mini-batch)</p></li><li><p><code>keepProb::AbstractFloat</code> := the keep probability (rate) of the drop-out operation <code>&lt;1.0</code></p></li><li><p><code>W::Array{T,2} where {T}</code> := the scaling parameters of this layer <code>W * X</code>, of the shape (channels of this layer, channels of the previous layer)</p></li><li><p><code>B::Array{T,2} where {T}</code> := the bias of this layer <code>W * X .+ B</code>, of the shape (channels of this layer, 1)</p></li><li><p><code>dW::Array{T,2} where {T}</code> := the derivative of the loss function to the W parameters <span>$\frac{dJ}{dW}$</span></p></li><li><p><code>dB::Array{T,2} where {T}</code> := the derivative of the loss function to the B parameters <span>$\frac{dJ}{dB}$</span></p></li><li><p><code>forwCount::Integer</code> := forward propagation counter</p></li><li><p><code>backCount::Integer</code> := backward propagation counter</p></li><li><p><code>updateCount::Integer</code> := update parameters counter</p></li><li><p><code>prevLayer::L where {L&lt;:Union{Layer,Nothing}}</code> := the previous layer which is</p></li></ul><p>the input of this layer</p><ul><li><code>nextLayers::Array{Layer,1}</code> := An array of the next <code>layer</code>(s)</li></ul><hr/><p><strong>Supertype Hierarchy</strong></p><pre><code class="language-none">FCLayer &lt;: Layer &lt;: Any</code></pre><hr/><p><strong>Examples</strong></p><pre><code class="language-julia">X_Input = Input(X_train)
X = FCLayer(20, :relu)(X_Input)
</code></pre><p>In the previous example the variable <code>X_Input</code> is a pointer to the <code>Input</code> layer, and <code>X</code> is an pointer to the <code>FCLayer(20, :relu)</code> layer. <strong>Note</strong> that the layer instance can be used as a connecting function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/TypeDef.jl#L12-L85">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.FCLayer-Tuple{Array}" href="#NumNN.FCLayer-Tuple{Array}"><code>NumNN.FCLayer</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">define input as X</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/chain.jl#L138-L140">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.FCLayer-Tuple{Layer}" href="#NumNN.FCLayer-Tuple{Layer}"><code>NumNN.FCLayer</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">connect with the previous layer</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/chain.jl#L33-L35">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.Flatten" href="#NumNN.Flatten"><code>NumNN.Flatten</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Flatten()</code></pre><p>Flatten the input into 2D <code>Array</code></p><p><strong>Summary</strong></p><pre><code class="language-none">mutable struct Flatten &lt;: Layer</code></pre><p><strong>Fields</strong></p><pre><code class="language-none">channels    :: Integer
inputS      :: Tuple
outputS     :: Tuple
forwCount   :: Integer
backCount   :: Integer
updateCount :: Integer
nextLayers  :: Array{Layer,1}
prevLayer   :: Union{Nothing, Layer}</code></pre><p><strong>Supertype Hierarchy</strong></p><pre><code class="language-none">Flatten &lt;: Layer &lt;: Any</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/TypeDef.jl#L670-L693">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.Input" href="#NumNN.Input"><code>NumNN.Input</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Input(X_shape::Tuple)</code></pre><p><code>Input</code> <code>Layer</code> that is used as a pointer to the input array(s).</p><p><strong>Arguments</strong></p><ul><li><code>X_shape::Tuple</code> := shape of the input Array</li></ul><hr/><p><strong>Summary</strong></p><pre><code class="language-none">mutable struct Input &lt;: Layer</code></pre><p><strong>Fields</strong></p><ul><li><p><code>channels::Integer</code> := is the number of nodes or <code>channels</code> in the layer</p></li><li><p><code>inputS::Tuple</code> := input size of the layer</p></li><li><p><code>outputS::Tuple</code> := output size of the layer</p></li><li><p><code>forwCount::Integer</code> := forward propagation counter</p></li><li><p><code>backCount::Integer</code> := backward propagation counter</p></li><li><p><code>updateCount::Integer</code> := update parameters counter</p></li><li><p><code>nextLayers::Array{Layer,1}</code> := An array of the next <code>layer</code>(s)</p></li><li><p><code>prevLayer::Array{Layer,1}</code> := An array of the previous <code>layer</code>(s) to be added</p></li></ul><hr/><p><strong>Supertype Hierarchy</strong></p><pre><code class="language-none">Input &lt;: Layer &lt;: Any</code></pre><hr/><p><strong>Examples</strong></p><pre><code class="language-julia">X_Input = Input(size(X_train))
X = FCLayer(10, :relu)(X_Input)</code></pre><p>It is possible to use the Array instead of its size <code>NumNN</code> will take care of the rest</p><pre><code class="language-julia">X_Input = Input(X_train)
X = FCLayer(10, :relu)(X_Input)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/TypeDef.jl#L416-L472">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.Layer" href="#NumNN.Layer"><code>NumNN.Layer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">abstract type to include all layers</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/TypeDef.jl#L2-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.MaxPool1D" href="#NumNN.MaxPool1D"><code>NumNN.MaxPool1D</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MaxPool1D(
    f::Integer=2;
    prevLayer=nothing,
    strides::Integer=f,
    padding::Symbol=:valid,
)</code></pre><p><strong>Summary</strong></p><pre><code class="language-none">mutable struct MaxPool1D &lt;: MaxPoolLayer</code></pre><p><strong>Fields</strong></p><pre><code class="language-none">channels    :: Integer
f           :: Integer
s           :: Integer
inputS      :: Tuple
outputS     :: Tuple
padding     :: Symbol
forwCount   :: Integer
backCount   :: Integer
updateCount :: Integer
prevLayer   :: Union{Nothing, Layer}
nextLayers  :: Array{Layer,1}</code></pre><p><strong>Supertype Hierarchy</strong></p><pre><code class="language-none">MaxPool1D &lt;: MaxPoolLayer &lt;: PoolLayer &lt;: PaddableLayer &lt;: Layer &lt;: Any</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/TypeDef.jl#L605-L634">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.MaxPool2D" href="#NumNN.MaxPool2D"><code>NumNN.MaxPool2D</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MaxPool2D(
    f::Tuple{Integer,Integer}=(2,2);
    prevLayer=nothing,
    strides::Tuple{Integer,Integer}=f,
    padding::Symbol=:valid,
)</code></pre><p><strong>Summary</strong></p><pre><code class="language-none">mutable struct MaxPool2D &lt;: MaxPoolLayer</code></pre><p><strong>Fields</strong></p><pre><code class="language-none">channels    :: Integer
f           :: Tuple{Integer,Integer}
s           :: Tuple{Integer,Integer}
inputS      :: Tuple
outputS     :: Tuple
padding     :: Symbol
forwCount   :: Integer
backCount   :: Integer
updateCount :: Integer
prevLayer   :: Union{Nothing, Layer}
nextLayers  :: Array{Layer,1}</code></pre><p><strong>Supertype Hierarchy</strong></p><pre><code class="language-none">MaxPool2D &lt;: MaxPoolLayer &lt;: PoolLayer &lt;: PaddableLayer &lt;: Layer &lt;: Any</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/TypeDef.jl#L511-L540">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.MaxPool3D" href="#NumNN.MaxPool3D"><code>NumNN.MaxPool3D</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MaxPool3D(
    f::Tuple{Integer,Integer,Integer}=(2,2,2);
    prevLayer=nothing,
    strides::Tuple{Integer,Integer,Integer}=f,
    padding::Symbol=:valid,
)</code></pre><p><strong>Summary</strong></p><pre><code class="language-none">mutable struct MaxPool3D &lt;: MaxPoolLayer</code></pre><p><strong>Fields</strong></p><pre><code class="language-none">channels    :: Integer
f           :: Tuple{Integer,Integer,Integer}
s           :: Tuple{Integer,Integer,Integer}
inputS      :: Tuple
outputS     :: Tuple
padding     :: Symbol
forwCount   :: Integer
backCount   :: Integer
updateCount :: Integer
prevLayer   :: Union{Nothing, Layer}
nextLayers  :: Array{Layer,1}</code></pre><p><strong>Supertype Hierarchy</strong></p><pre><code class="language-none">MaxPool3D &lt;: MaxPoolLayer &lt;: PoolLayer &lt;: PaddableLayer &lt;: Layer &lt;: Any</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/TypeDef.jl#L699-L728">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.MaxPoolLayer" href="#NumNN.MaxPoolLayer"><code>NumNN.MaxPoolLayer</code></a> — <span class="docstring-category">Type</span></header><section><div><p><strong>Summary</strong></p><pre><code class="language-none">abstract type MaxPoolLayer &lt;: PoolLayer</code></pre><p>Abstract Type to hold all the <code>MaxPoolLayer</code>s</p><p><strong>Subtypes</strong></p><pre><code class="language-none">MaxPool1D
MaxPool2D
MaxPool3D</code></pre><p><strong>Supertype Hierarchy</strong></p><pre><code class="language-none">MaxPoolLayer &lt;: PoolLayer &lt;: PaddableLayer &lt;: Layer &lt;: Any</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/TypeDef.jl#L490-L506">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.Model" href="#NumNN.Model"><code>NumNN.Model</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">function Model(
    X,
    Y,
    inLayer::Layer,
    outLayer::Layer,
    α;
    optimizer = :gds,
    β1 = 0.9,
    β2 = 0.999,
    ϵAdam = 1e-8,
    regulization = 0,
    λ = 1.0,
    lossFun = :categoricalCrossentropy,
    paramsDtype::DataType = Float64,
)</code></pre><p><strong>Summary</strong></p><pre><code class="language-none">mutable struct Model &lt;: Any</code></pre><p><strong>Fields</strong></p><pre><code class="language-none">inLayer      :: Layer
outLayer     :: Layer
lossFun      :: Symbol
paramsDtype  :: DataType
regulization :: Integer
λ            :: AbstractFloat
α            :: AbstractFloat
optimizer    :: Symbol
ϵAdam        :: AbstractFloat
β1           :: AbstractFloat
β2           :: AbstractFloat</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/TypeDef.jl#L726-L761">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.PoolLayer" href="#NumNN.PoolLayer"><code>NumNN.PoolLayer</code></a> — <span class="docstring-category">Type</span></header><section><div><p><strong>Summary</strong></p><pre><code class="language-none">abstract type PoolLayer &lt;: PaddableLayer</code></pre><p>Abstract Type to hold all the <code>PoolLayer</code>s</p><p><strong>Subtypes</strong></p><pre><code class="language-none">AveragePoolLayer
MaxPoolLayer</code></pre><p><strong>Supertype Hierarchy</strong></p><pre><code class="language-none">PoolLayer &lt;: PaddableLayer &lt;: Layer &lt;: Any</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/TypeDef.jl#L470-L485">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.binaryCrossentropy" href="#NumNN.binaryCrossentropy"><code>NumNN.binaryCrossentropy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">return the average cross entropy loss over vector of labels and predictions

input:
    a := (?1, c,m) matrix of predicted values, where c is the number of classes
    y := (?1, c,m) matrix of predicted values, where c is the number of classes

    Note: in case the number of classes is one (1) it is okay to have
          a scaler values for a and y

output:
    J := scaler value of the cross entropy loss</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/lossFuns.jl#L8-L20">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.relu-Union{Tuple{AbstractArray{T,N}}, Tuple{N}, Tuple{T}} where N where T" href="#NumNN.relu-Union{Tuple{AbstractArray{T,N}}, Tuple{N}, Tuple{T}} where N where T"><code>NumNN.relu</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">return the ReLU output</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/actFuns.jl#L70-L72">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.softmax-Union{Tuple{AbstractArray{T,N}}, Tuple{N}, Tuple{T}} where N where T" href="#NumNN.softmax-Union{Tuple{AbstractArray{T,N}}, Tuple{N}, Tuple{T}} where N where T"><code>NumNN.softmax</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">compute the softmax function</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/actFuns.jl#L97-L100">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.σ-Tuple{Any,Any,Any}" href="#NumNN.σ-Tuple{Any,Any,Any}"><code>NumNN.σ</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">return the Sigmoid output
inputs must be matices</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/actFuns.jl#L15-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.NNConv-Union{Tuple{CL}, Tuple{N}, Tuple{T}, Tuple{CL,AbstractArray{T,N}}} where CL&lt;:ConvLayer where N where T" href="#NumNN.NNConv-Union{Tuple{CL}, Tuple{N}, Tuple{T}, Tuple{CL,AbstractArray{T,N}}} where CL&lt;:ConvLayer where N where T"><code>NumNN.NNConv</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">NNConv(cLayer::CL, Ai::AbstractArray{T,N}) where {T,N, CL &lt;: ConvLayer}</code></pre><p>Perform the forward propagation for <code>cLayer::ConvLayer</code> using fast implementation of <code>NNlib</code></p><p><strong>Return</strong></p><ul><li><code>Dict(:Z =&gt; Z, :A =&gt; A)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/parallelNNConv.jl#L5-L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.chain-Union{Tuple{L}, Tuple{Any,Array{L,1}}} where L&lt;:Layer" href="#NumNN.chain-Union{Tuple{L}, Tuple{Any,Array{L,1}}} where L&lt;:Layer"><code>NumNN.chain</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">function chain(X, arr::Array{L,1}) where {L&lt;:Layer}</code></pre><p>Returns the input <code>Layer</code> and the output <code>Layer</code> from an <code>Array</code> of layers and the input of the model as and <code>Array</code> <code>X</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/chain.jl#L3-L8">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.chainBackProp-Union{Tuple{N2}, Tuple{N1}, Tuple{T2}, Tuple{T1}, Tuple{L}, Tuple{AbstractArray{T1,N1},AbstractArray{T2,N2},Model,Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{AbstractArray{T1,N1},AbstractArray{T2,N2},Model,Dict{Layer,Dict{Symbol,AbstractArray}},L}, Tuple{AbstractArray{T1,N1},AbstractArray{T2,N2},Model,Dict{Layer,Dict{Symbol,AbstractArray}},L,Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{AbstractArray{T1,N1},AbstractArray{T2,N2},Model,Dict{Layer,Dict{Symbol,AbstractArray}},L,Dict{Layer,Dict{Symbol,AbstractArray}},Any}} where N2 where N1 where T2 where T1 where L&lt;:Union{Nothing, Layer}" href="#NumNN.chainBackProp-Union{Tuple{N2}, Tuple{N1}, Tuple{T2}, Tuple{T1}, Tuple{L}, Tuple{AbstractArray{T1,N1},AbstractArray{T2,N2},Model,Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{AbstractArray{T1,N1},AbstractArray{T2,N2},Model,Dict{Layer,Dict{Symbol,AbstractArray}},L}, Tuple{AbstractArray{T1,N1},AbstractArray{T2,N2},Model,Dict{Layer,Dict{Symbol,AbstractArray}},L,Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{AbstractArray{T1,N1},AbstractArray{T2,N2},Model,Dict{Layer,Dict{Symbol,AbstractArray}},L,Dict{Layer,Dict{Symbol,AbstractArray}},Any}} where N2 where N1 where T2 where T1 where L&lt;:Union{Nothing, Layer}"><code>NumNN.chainBackProp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">function chainBackProp(
    X::AbstractArray{T1,N1},
    Y::AbstractArray{T2,N2},
    model::Model,
    FCache::Dict{Layer,Dict{Symbol,AbstractArray}},
    cLayer::L = nothing,
    BCache::Dict{Layer,Dict{Symbol,AbstractArray}}=Dict{Layer,Dict{Symbol,AbstractArray}}(),
    cnt = -1;
    tMiniBatch::Integer = -1, #can be used to perform both back and update params
    kwargs...,
) where {L&lt;:Union{Layer,Nothing},T1,T2,N1,N2}</code></pre><p><strong>Arguments</strong></p><ul><li><p><code>X</code> := train data</p></li><li><p><code>Y</code> := train labels</p></li><li><p><code>model</code> := is the model to perform the back propagation on</p></li><li><p><code>FCache</code> := the cached values of the forward propagation as <code>Dict{Layer, Dict{Symbol, AbstractArray}}</code></p></li><li><p><code>cLayer</code> := is an internal variable to hold the current layer</p></li><li><p><code>BCache</code> := to hold the cache of the back propagtion (internal variable)</p></li><li><p><code>cnt</code> := is an internal variable to count the step of back propagation currently on to avoid re-do it</p></li></ul><p><strong>Key-word Arguments</strong></p><ul><li><p><code>tMiniBatch</code> := to perform both the back prop and update trainable parameters in the same recursive call (if less than 1 update during back propagation is ditched)</p></li><li><p><code>kwargs</code> := other key-word arguments to be bassed to <code>layerBackProp</code> methods</p></li></ul><p><strong>Return</strong></p><ul><li><code>BCache</code> := the cached values of the back propagation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/parallelBackForProp.jl#L270-L310">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.chainForProp-Union{Tuple{N}, Tuple{T}, Tuple{AbstractArray{T,N},Layer}, Tuple{AbstractArray{T,N},Layer,Integer}} where N where T" href="#NumNN.chainForProp-Union{Tuple{N}, Tuple{T}, Tuple{AbstractArray{T,N},Layer}, Tuple{AbstractArray{T,N},Layer,Integer}} where N where T"><code>NumNN.chainForProp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">function chainForProp(
    X::AbstractArray{T,N},
    cLayer::Layer,
    cnt::Integer = -1;
    FCache = Dict{Layer,Dict{Symbol,AbstractArray}}(),
    kwargs...,
) where {T,N}</code></pre><p>perform the chained forward propagation using recursive calls</p><p><strong>Arguments:</strong></p><ul><li><p><code>X::AbstractArray{T,N}</code> := input of the input layer</p></li><li><p><code>cLayer::Layer</code> := Input Layer</p></li><li><p><code>cnt::Integer</code> := an internal counter used to cache the layers was performed not to redo it again</p></li></ul><p><strong>Returns</strong></p><ul><li><code>Cache::Dict{Layer, Dict{Symbol, Array}}</code> := the output each layer either A, Z or together As Dict of layer to dict of Symbols and Arrays for internal use, it set again the values of Z and A in each layer to be used later in back propagation and add one to the layer forwCount value when pass through it</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/parallelBackForProp.jl#L16-L38">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.chainUpdateParams!-Union{Tuple{Model}, Tuple{L}, Tuple{Model,L}, Tuple{Model,L,Any}} where L&lt;:Union{Nothing, Layer}" href="#NumNN.chainUpdateParams!-Union{Tuple{Model}, Tuple{L}, Tuple{Model,L}, Tuple{Model,L,Any}} where L&lt;:Union{Nothing, Layer}"><code>NumNN.chainUpdateParams!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">chainUpdateParams!(model::Model,
                   cLayer::L=nothing,
                   cnt = -1;
                   tMiniBatch::Integer = 1) where {L&lt;:Union{Layer,Nothing}}</code></pre><p>Update trainable parameters using recursive call</p><p><strong>Arguments</strong></p><ul><li><p><code>model</code> := the model holds the training and update process</p></li><li><p><code>cLayer</code> := internal variable for recursive call holds the current layer</p></li><li><p><code>cnt</code> := an internal variable to hold the count of update in each layer not to re-do it</p></li></ul><p><strong>Key-word Arguments</strong></p><ul><li><code>tMiniBatch</code> := the number of mini-batch of the total train collection</li></ul><p><strong>Return</strong></p><ul><li><code>nothing</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/parallelBackForProp.jl#L427-L451">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.dNNConv!-Union{Tuple{CL}, Tuple{N}, Tuple{T3}, Tuple{T2}, Tuple{T1}, Tuple{CL,AbstractArray{T1,N},AbstractArray{T2,N},AbstractArray{T3,N}}} where CL&lt;:ConvLayer where N where T3 where T2 where T1" href="#NumNN.dNNConv!-Union{Tuple{CL}, Tuple{N}, Tuple{T3}, Tuple{T2}, Tuple{T1}, Tuple{CL,AbstractArray{T1,N},AbstractArray{T2,N},AbstractArray{T3,N}}} where CL&lt;:ConvLayer where N where T3 where T2 where T1"><code>NumNN.dNNConv!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">function dNNConv!(
    cLayer::CL,
    Ai::AbstractArray{T1,N},
    dAi::AbstractArray{T2,N},
    dZ::AbstractArray{T3,N},
) where {T1, T2, T3, N, CL &lt;: ConvLayer}</code></pre><p>Performs the back propagation for <code>cLayer::ConvLayer</code> and save values to the pre-allocated <code>Array</code> <code>dAi</code> and trainable parameters <code>W</code> &amp; <code>B</code></p><p><strong>Arguments</strong></p><ul><li><p><code>cLayer::ConvLayer</code></p></li><li><p><code>Ai::AbstractArray{T1,N}</code> := the input activation of <code>cLayer</code></p></li><li><p><code>dAi::AbstractArray{T2,N}</code> := pre-allocated to hold the derivative of the activation</p></li><li><p><code>dZ::AbstractArray{T3,N}</code> := the derivative of the cost to the input of the activation function</p></li></ul><p><strong>Return</strong></p><p><code>nothing</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/parallelNNConv.jl#L35-L59">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.dbinaryCrossentropy-Tuple{Any,Any}" href="#NumNN.dbinaryCrossentropy-Tuple{Any,Any}"><code>NumNN.dbinaryCrossentropy</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">compute the drivative of cross-entropy loss function to the input of the
layer dZ</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/lossFuns.jl#L32-L35">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.deepInitWB!" href="#NumNN.deepInitWB!"><code>NumNN.deepInitWB!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">initialize W&#39;s and B&#39;s using

inputs:
    X := is the input of the neural Network
    outLayer := is the output Layer or the current layer
                of initialization
    cnt := is a counter to determinde the current step
            and its an internal variable


    kwargs:
        He := is a true/false array, whether to use the He **et al.** initialization
                or not

        coef := when not using He **et al.** initialization use this coef
                to multiply with the random numbers initialization
        zro := true/false variable whether to initialize W with zeros or not</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/initializations.jl#L135-L154">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.drelu-Union{Tuple{AbstractArray{T,N}}, Tuple{N}, Tuple{T}} where N where T" href="#NumNN.drelu-Union{Tuple{AbstractArray{T,N}}, Tuple{N}, Tuple{T}} where N where T"><code>NumNN.drelu</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">return the derivative of ReLU function</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/actFuns.jl#L79-L81">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.dσ-Tuple{Any}" href="#NumNN.dσ-Tuple{Any}"><code>NumNN.dσ</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">return the derivative of Sigmoid function</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/actFuns.jl#L24-L26">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.getLayerSlice-Tuple{Layer,Layer,Dict{Layer,Dict{Symbol,AbstractArray}}}" href="#NumNN.getLayerSlice-Tuple{Layer,Layer,Dict{Layer,Dict{Symbol,AbstractArray}}}"><code>NumNN.getLayerSlice</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">getLayerSlice(cLayer::Layer, nextLayer::Layer, BCache::Dict{Layer, Dict{Symbol, AbstractArray}})</code></pre><p>Fall back method for  <code>Layer</code>s other than <code>ConcatLayer</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/assistance.jl#L126-L131">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.initWB!-Union{Tuple{FCLayer}, Tuple{T}, Tuple{FCLayer,Type{T}}} where T" href="#NumNN.initWB!-Union{Tuple{FCLayer}, Tuple{T}, Tuple{FCLayer,Type{T}}} where T"><code>NumNN.initWB!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">initialize W and B for layer with inputs of size of (nl_1) and layer size
    of (nl)

returns:
    W: of size of (nl, nl_1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/initializations.jl#L6-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.layerBackProp" href="#NumNN.layerBackProp"><code>NumNN.layerBackProp</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">layerBackProp(
    cLayer::BatchNorm,
    model::Model,
    FCache::Dict{Layer, Dict{Symbol, AbstractArray}},
    BCache::Dict{Layer, Dict{Symbol, AbstractArray}},
    dAo::AbstractArray = Array{Any,1}(undef,0);
    labels::AbstractArray = Array{Any,1}(undef,0),
    kwargs...,
)</code></pre><p>Perform the back propagation of <code>BatchNorm</code> type on the activations and trainable parameters <code>W</code> and <code>B</code></p><p><strong>Argument</strong></p><ul><li><p><code>cLayer</code> := the layer to perform the backprop on</p></li><li><p><code>model</code> := the <code>Model</code></p></li><li><p><code>FCache</code> := the cache values of the forprop</p></li><li><p><code>BCache</code> := the cache values of the backprop from the front <code>Layer</code>(s)</p></li><li><p><code>dAo</code> := (for test purpose) the derivative of the front <code>Layer</code></p></li><li><p><code>labels</code> := in case this is the output layer</p></li></ul><p><strong>Return</strong></p><ul><li>A <code>Dict{Symbol, AbstractArray}(:dA =&gt; dAi)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/parallelLayerBackProp.jl#L554-L584">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.layerBackProp-Union{Tuple{CL}, Tuple{CL,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{CL,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray}, Tuple{CL,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray,AbstractArray}, Tuple{CL,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray,AbstractArray,AbstractArray}} where CL&lt;:ConvLayer" href="#NumNN.layerBackProp-Union{Tuple{CL}, Tuple{CL,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{CL,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray}, Tuple{CL,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray,AbstractArray}, Tuple{CL,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray,AbstractArray,AbstractArray}} where CL&lt;:ConvLayer"><code>NumNN.layerBackProp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">function layerBackProp(
    cLayer::ConvLayer,
    model::Model,
    FCache::Dict{Layer, Dict{Symbol, AbstractArray}},
    BCache::Dict{Layer, Dict{Symbol, AbstractArray}},
    Ai::AbstractArray = Array{Any,1}(undef,0),
    Ao::AbstractArray = Array{Any,1}(undef,0),
    dAo::AbstractArray = Array{Any,1}(undef,0);
    labels::AbstractArray = Array{Any,1}(undef,0),
    kwargs...
)</code></pre><p>Performs the layer back propagation for a <code>ConvLayer</code></p><p><strong>Arguments</strong></p><ul><li><p><code>cLayer::ConvLayer</code></p></li><li><p><code>model::Model</code></p></li><li><p><code>FCache</code> := the cache of the forward propagation step</p></li><li><p><code>BCache</code> := the cache of so far done back propagation</p></li><li><p>for test purpose   Ai   Ao   dAo</p></li><li><p><code>labels</code> := when <code>cLayer</code> is an output <code>Layer</code></p></li></ul><p><strong>Return</strong></p><ul><li><code>Dict(:dA =&gt; dAi)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/parallelLayerBackProp.jl#L38-L74">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.layerBackProp-Union{Tuple{N}, Tuple{T2}, Tuple{T1}, Tuple{Activation,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{Activation,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray{T1,N}}} where N where T2 where T1" href="#NumNN.layerBackProp-Union{Tuple{N}, Tuple{T2}, Tuple{T1}, Tuple{Activation,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{Activation,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray{T1,N}}} where N where T2 where T1"><code>NumNN.layerBackProp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">layerBackProp(
    cLayer::Activation,
    model::Model,
    FCache::Dict{Layer, Dict{Symbol, AbstractArray}},
    BCache::Dict{Layer, Dict{Symbol, AbstractArray}},
    dAo::AbstractArray{T1,N} = Array{Any,1}(undef,0);
    labels::AbstractArray{T2,N} = Array{Any,1}(undef,0),
    kwargs...,
) where {T1,T2,N}</code></pre><p>Perform the back propagation of <code>Activation</code> type</p><p><strong>Argument</strong></p><ul><li><p><code>cLayer</code> := the layer to perform the backprop on</p></li><li><p><code>model</code> := the <code>Model</code></p></li><li><p><code>FCache</code> := the cache values of the forprop</p></li><li><p><code>BCache</code> := the cache values of the backprop from the front <code>Layer</code>(s)</p></li><li><p><code>dAo</code> := (for test purpose) the derivative of the front <code>Layer</code></p></li><li><p><code>labels</code> := in case this is the output layer</p></li></ul><p><strong>Return</strong></p><ul><li>A <code>Dict{Symbol, AbstractArray}(:dA =&gt; dAi)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/parallelLayerBackProp.jl#L184-L214">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.layerBackProp-Union{Tuple{N}, Tuple{T2}, Tuple{T1}, Tuple{AddLayer,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{AddLayer,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray{T1,N}}} where N where T2 where T1" href="#NumNN.layerBackProp-Union{Tuple{N}, Tuple{T2}, Tuple{T1}, Tuple{AddLayer,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{AddLayer,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray{T1,N}}} where N where T2 where T1"><code>NumNN.layerBackProp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">layerBackProp(
    cLayer::AddLayer,
    model::Model,
    FCache::Dict{Layer, Dict{Symbol, AbstractArray}},
    BCache::Dict{Layer, Dict{Symbol, AbstractArray}},
    dAo::AbstractArray{T1,N} = Array{Any,1}(undef,0);
    labels::AbstractArray{T2,N} = Array{Any,1}(undef,0),
    kwargs...,
) where {T1,T2,N}</code></pre><p>Perform the back propagation of <code>AddLayer</code> type</p><p><strong>Argument</strong></p><ul><li><p><code>cLayer</code> := the layer to perform the backprop on</p></li><li><p><code>model</code> := the <code>Model</code></p></li><li><p><code>FCache</code> := the cache values of the forprop</p></li><li><p><code>BCache</code> := the cache values of the backprop from the front <code>Layer</code>(s)</p></li><li><p><code>dAo</code> := (for test purpose) the derivative of the front <code>Layer</code></p></li><li><p><code>labels</code> := in case this is the output layer</p></li></ul><p><strong>Return</strong></p><ul><li>A <code>Dict{Symbol, AbstractArray}(:dA =&gt; dAi)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/parallelLayerBackProp.jl#L380-L410">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.layerBackProp-Union{Tuple{N}, Tuple{T2}, Tuple{T1}, Tuple{Flatten,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{Flatten,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray{T1,N}}} where N where T2 where T1" href="#NumNN.layerBackProp-Union{Tuple{N}, Tuple{T2}, Tuple{T1}, Tuple{Flatten,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{Flatten,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray{T1,N}}} where N where T2 where T1"><code>NumNN.layerBackProp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">layerBackProp(
    cLayer::Flatten,
    model::Model,
    FCache::Dict{Layer, Dict{Symbol, AbstractArray}},
    BCache::Dict{Layer, Dict{Symbol, AbstractArray}},
    dAo::AbstractArray{T1,N} = Array{Any,1}(undef,0);
    labels::AbstractArray{T2,N} = Array{Any,1}(undef,0),
    kwargs...,
) where {T1,T2,N}</code></pre><p>Perform the back propagation of <code>Flatten</code> type</p><p><strong>Argument</strong></p><ul><li><p><code>cLayer</code> := the layer to perform the backprop on</p></li><li><p><code>model</code> := the <code>Model</code></p></li><li><p><code>FCache</code> := the cache values of the forprop</p></li><li><p><code>BCache</code> := the cache values of the backprop from the front <code>Layer</code>(s)</p></li><li><p><code>dAo</code> := (for test purpose) the derivative of the front <code>Layer</code></p></li><li><p><code>labels</code> := in case this is the output layer</p></li></ul><p><strong>Return</strong></p><ul><li>A <code>Dict{Symbol, AbstractArray}(:dA =&gt; dAi)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/parallelLayerBackProp.jl#L301-L331">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.layerBackProp-Union{Tuple{N}, Tuple{T2}, Tuple{T1}, Tuple{Input,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{Input,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray{T1,N}}} where N where T2 where T1" href="#NumNN.layerBackProp-Union{Tuple{N}, Tuple{T2}, Tuple{T1}, Tuple{Input,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{Input,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray{T1,N}}} where N where T2 where T1"><code>NumNN.layerBackProp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">layerBackProp(
    cLayer::Input,
    model::Model,
    FCache::Dict{Layer, Dict{Symbol, AbstractArray}},
    BCache::Dict{Layer, Dict{Symbol, AbstractArray}},
    dAo::AbstractArray{T1,N} = Array{Any,1}(undef,0);
    labels::AbstractArray{T2,N} = Array{Any,1}(undef,0),
    kwargs...,
) where {T1,T2,N}</code></pre><p>Perform the back propagation of <code>Input</code> type</p><p><strong>Argument</strong></p><ul><li><p><code>cLayer</code> := the layer to perform the backprop on</p></li><li><p><code>model</code> := the <code>Model</code></p></li><li><p><code>FCache</code> := the cache values of the forprop</p></li><li><p><code>BCache</code> := the cache values of the backprop from the front <code>Layer</code>(s)</p></li><li><p><code>dAo</code> := (for test purpose) the derivative of the front <code>Layer</code></p></li><li><p><code>labels</code> := in case this is the output layer</p></li></ul><p><strong>Return</strong></p><ul><li>A <code>Dict{Symbol, AbstractArray}(:dA =&gt; dAi)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/parallelLayerBackProp.jl#L488-L518">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.layerBackProp-Union{Tuple{PL}, Tuple{PL,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{PL,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray}, Tuple{PL,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray,AbstractArray}, Tuple{PL,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray,AbstractArray,AbstractArray}} where PL&lt;:PoolLayer" href="#NumNN.layerBackProp-Union{Tuple{PL}, Tuple{PL,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{PL,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray}, Tuple{PL,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray,AbstractArray}, Tuple{PL,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray,AbstractArray,AbstractArray}} where PL&lt;:PoolLayer"><code>NumNN.layerBackProp</code></a> — <span class="docstring-category">Method</span></header><section><div><p>layerBackProp(     cLayer::PoolLayer,     model::Model,     FCache::Dict{Layer, Dict{Symbol, AbstractArray}},     BCache::Dict{Layer, Dict{Symbol, AbstractArray}},     Ai::AbstractArray = Array{Any,1}(undef,0),     Ao::AbstractArray = Array{Any,1}(undef,0),     dAo::AbstractArray = Array{Any,1}(undef,0);     labels::AbstractArray = Array{Any,1}(undef,0),     kwargs... )</p><p>Performs the layer back propagation for a <code>PoolLayer</code></p><p><strong>Arguments</strong></p><ul><li><p><code>cLayer::ConvLayer</code></p></li><li><p><code>model::Model</code></p></li><li><p><code>FCache</code> := the cache of the forward propagation step</p></li><li><p><code>BCache</code> := the cache of so far done back propagation</p></li><li><p>for test purpose   Ai   Ao   dAo</p></li><li><p><code>labels</code> := when <code>cLayer</code> is an output <code>Layer</code></p></li></ul><p><strong>Return</strong></p><ul><li><code>Dict(:dA =&gt; dAi)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/parallelLayerBackProp.jl#L158-L194">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.layerBackProp-Union{Tuple{SoS}, Tuple{Activation,Model,SoS,AbstractArray,AbstractArray}} where SoS&lt;:Union{Type{softmax}, Type{σ}}" href="#NumNN.layerBackProp-Union{Tuple{SoS}, Tuple{Activation,Model,SoS,AbstractArray,AbstractArray}} where SoS&lt;:Union{Type{softmax}, Type{σ}}"><code>NumNN.layerBackProp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">layerBackProp(
    cLayer::Activation,
    model::Model,
    actFun::SoS,
    Ao::AbstractArray,
    labels::AbstractArray,
) where {SoS&lt;:Union{Type{softmax},Type{σ}}}</code></pre><p>For output <code>Activation</code> layers with softmax and sigmoid activation functions</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/parallelLayerBackProp.jl#L33-L43">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.layerBackProp-Union{Tuple{SoS}, Tuple{ConvLayer,Model,SoS,AbstractArray,AbstractArray}} where SoS&lt;:Union{Type{softmax}, Type{σ}}" href="#NumNN.layerBackProp-Union{Tuple{SoS}, Tuple{ConvLayer,Model,SoS,AbstractArray,AbstractArray}} where SoS&lt;:Union{Type{softmax}, Type{σ}}"><code>NumNN.layerBackProp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">function layerBackProp(
    cLayer::ConvLayer,
    model::Model,
    actFun::SoS,
    Ao::AbstractArray,
    labels::AbstractArray,
) where {SoS&lt;:Union{Type{softmax},Type{σ}}}</code></pre><p>Derive the loss function to the input of the activation function when activation is either <code>softmax</code> or <code>σ</code></p><p><strong>Return</strong></p><ul><li><code>dZ::AbstractArray</code> := the derivative of the loss function to the input of the activation function</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/parallelLayerBackProp.jl#L5-L20">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.layerBackProp-Union{Tuple{SoS}, Tuple{FCLayer,Model,SoS,AbstractArray,AbstractArray}} where SoS&lt;:Union{Type{softmax}, Type{σ}}" href="#NumNN.layerBackProp-Union{Tuple{SoS}, Tuple{FCLayer,Model,SoS,AbstractArray,AbstractArray}} where SoS&lt;:Union{Type{softmax}, Type{σ}}"><code>NumNN.layerBackProp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">layerBackProp(
    cLayer::FCLayer,
    model::Model,
    actFun::SoS,
    Ao::AbstractArray,
    labels::AbstractArray,
) where {SoS&lt;:Union{Type{softmax},Type{σ}}}</code></pre><p>For output <code>FCLayer</code> layers with softmax and sigmoid activation functions</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/parallelLayerBackProp.jl#L2-L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.layerBackProp-Union{Tuple{T2}, Tuple{T1}, Tuple{FCLayer,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{FCLayer,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray{T1,2}}} where T2 where T1" href="#NumNN.layerBackProp-Union{Tuple{T2}, Tuple{T1}, Tuple{FCLayer,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{FCLayer,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray{T1,2}}} where T2 where T1"><code>NumNN.layerBackProp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">layerBackProp(
    cLayer::FCLayer,
    model::Model,
    FCache::Dict{Layer, Dict{Symbol, AbstractArray}},
    BCache::Dict{Layer, Dict{Symbol, AbstractArray}},
    dAo::AbstractArray{T1,2} = Array{Any,2}(undef,0,0);
    labels::AbstractArray{T2,2} = Array{Any,2}(undef,0,0),
    kwargs...,
) where {T1,T2}</code></pre><p>Perform the back propagation of <code>FCLayer</code> type on the activations and trainable parameters <code>W</code> and <code>B</code></p><p><strong>Argument</strong></p><ul><li><p><code>cLayer</code> := the layer to perform the backprop on</p></li><li><p><code>model</code> := the <code>Model</code></p></li><li><p><code>FCache</code> := the cache values of the forprop</p></li><li><p><code>BCache</code> := the cache values of the backprop from the front <code>Layer</code>(s)</p></li><li><p><code>dAo</code> := (for test purpose) the derivative of the front <code>Layer</code></p></li><li><p><code>labels</code> := in case this is the output layer</p></li></ul><p><strong>Return</strong></p><ul><li>A <code>Dict{Symbol, AbstractArray}(:dA =&gt; dAi)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/parallelLayerBackProp.jl#L61-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.layerForProp" href="#NumNN.layerForProp"><code>NumNN.layerForProp</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">layerForProp(
    cLayer::Flatten,
    Ai::AbstractArray = Array{Any,1}(undef,0);
    FCache::Dict{Layer,Dict{Symbol, AbstractArray}},
    kwargs...,
)</code></pre><p>Perform forward propagation for <code>Flatten</code> <code>Layer</code></p><p><strong>Arguments</strong></p><ul><li><p><code>cLayer</code> := the layer to perform for prop on</p></li><li><p><code>Ai</code> := is the input activation of the <code>Flatten</code> <code>Layer</code></p></li><li><p><code>FCache</code> := a cache holder of the for prop</p></li></ul><p><strong>Return</strong></p><ul><li>A <code>Dict{Symbol, AbstractArray}(:A =&gt; Ao)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/parallelLayerForProp.jl#L194-L215">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.layerForProp" href="#NumNN.layerForProp"><code>NumNN.layerForProp</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">layerForProp(
    cLayer::BatchNorm,
    Ai::AbstractArray = Array{Any,1}(undef,0);
    FCache::Dict{Layer,Dict{Symbol, AbstractArray}},
    kwargs...,
)</code></pre><p>Perform forward propagation for <code>BatchNorm</code> <code>Layer</code> and trainable parameters W and B</p><p><strong>Arguments</strong></p><ul><li><p><code>cLayer</code> := the layer to perform for prop on</p></li><li><p><code>Ai</code> := is the input activation of the <code>BatchNorm</code> <code>Layer</code></p></li><li><p><code>FCache</code> := a cache holder of the for prop</p></li></ul><p><strong>Return</strong></p><ul><li><code>Dict(       :μ =&gt; μ,       :Ai_μ =&gt; Ai_μ,       :Ai_μ_s =&gt; Ai_μ_s,       :var =&gt; var,       :Z =&gt; Z,       :A =&gt; Ao,       :Ap =&gt; Ap,       )</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/parallelLayerForProp.jl#L241-L271">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.layerForProp" href="#NumNN.layerForProp"><code>NumNN.layerForProp</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">layerForProp(
    cLayer::Input,
    X::AbstractArray = Array{Any,1}(undef,0);
    FCache::Dict{Layer,Dict{Symbol, AbstractArray}},
    kwargs...,
)</code></pre><p>Perform forward propagation for <code>Input</code> <code>Layer</code></p><p><strong>Arguments</strong></p><ul><li><p><code>cLayer</code> := the layer to perform for prop on</p></li><li><p><code>X</code> := is the input data of the <code>Input</code> <code>Layer</code></p></li><li><p><code>FCache</code> := a cache holder of the for prop</p></li></ul><p><strong>Return</strong></p><ul><li>A <code>Dict{Symbol, AbstractArray}(:A =&gt; Ao)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/parallelLayerForProp.jl#L5-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.layerForProp" href="#NumNN.layerForProp"><code>NumNN.layerForProp</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">layerForProp(
    cLayer::FCLayer,
    Ai::AbstractArray = Array{Any,1}(undef,0);
    FCache::Dict{Layer,Dict{Symbol, AbstractArray}},
    kwargs...,
)</code></pre><p>Perform forward propagation for <code>FCLayer</code> <code>Layer</code></p><p><strong>Arguments</strong></p><ul><li><p><code>cLayer</code> := the layer to perform for prop on</p></li><li><p><code>Ai</code> := is the input activation of the <code>FCLayer</code> <code>Layer</code></p></li><li><p><code>FCache</code> := a cache holder of the for prop</p></li></ul><p><strong>Return</strong></p><ul><li>A <code>Dict{Symbol, AbstractArray}(:A =&gt; Ao, :Z =&gt; Z)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/parallelLayerForProp.jl#L46-L67">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.layerForProp" href="#NumNN.layerForProp"><code>NumNN.layerForProp</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">layerForProp(
    cLayer::Activation,
    Ai::AbstractArray = Array{Any,1}(undef,0);
    FCache::Dict{Layer,Dict{Symbol, AbstractArray}},
    kwargs...,
)</code></pre><p>Perform forward propagation for <code>Activation</code> <code>Layer</code></p><p><strong>Arguments</strong></p><ul><li><p><code>cLayer</code> := the layer to perform for prop on</p></li><li><p><code>Ai</code> := is the input activation of the <code>Activation</code> <code>Layer</code></p></li><li><p><code>FCache</code> := a cache holder of the for prop</p></li></ul><p><strong>Return</strong></p><ul><li>A <code>Dict{Symbol, AbstractArray}(:A =&gt; Ao)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/parallelLayerForProp.jl#L151-L172">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.layerForProp-Tuple{AddLayer}" href="#NumNN.layerForProp-Tuple{AddLayer}"><code>NumNN.layerForProp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">layerForProp(
    cLayer::AddLayer;
    FCache::Dict{Layer,Dict{Symbol, AbstractArray}},
    kwargs...,
)</code></pre><p>Perform forward propagation for <code>AddLayer</code> <code>Layer</code></p><p><strong>Arguments</strong></p><ul><li><p><code>cLayer</code> := the layer to perform for prop on</p></li><li><p><code>FCache</code> := a cache holder of the for prop</p></li></ul><p><strong>Return</strong></p><ul><li>A <code>Dict{Symbol, AbstractArray}(:A =&gt; Ao)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/parallelLayerForProp.jl#L92-L110">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.layerForProp-Union{Tuple{CL}, Tuple{CL}, Tuple{CL,AbstractArray}} where CL&lt;:ConvLayer" href="#NumNN.layerForProp-Union{Tuple{CL}, Tuple{CL}, Tuple{CL,AbstractArray}} where CL&lt;:ConvLayer"><code>NumNN.layerForProp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">function layerForProp(
    cLayer::ConvLayer,
    Ai::AbstractArray = Array{Any,1}(undef,0);
    FCache::Dict{Layer,Dict{Symbol, AbstractArray}},
    kwargs...
)</code></pre><p>Perform the layer forward propagation for a <code>ConvLayer</code></p><p><strong>Arguments</strong></p><ul><li><p><code>cLayer::ConvLayer</code></p></li><li><p><code>Ai</code> := optional activation of the previous layer</p></li><li><p><code>FCache</code> := a <code>Dict</code> holds the outputs of <code>layerForProp</code> of the previous <code>Layer</code>(s)</p></li></ul><p><strong>Returns</strong></p><ul><li><code>Dict(:Z =&gt; Z, :A =&gt; Ao)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/parallelLayerForProp.jl#L7-L28">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.layerForProp-Union{Tuple{PL}, Tuple{PL}, Tuple{PL,AbstractArray}} where PL&lt;:PoolLayer" href="#NumNN.layerForProp-Union{Tuple{PL}, Tuple{PL}, Tuple{PL,AbstractArray}} where PL&lt;:PoolLayer"><code>NumNN.layerForProp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">layerForProp(
    cLayer::PoolLayer},
    Ai::AbstractArray = Array{Any,1}(undef,0);
    FCache::Dict{Layer,Dict{Symbol, AbstractArray}},
    kwargs...
)</code></pre><p>Perform the layer forward propagation for a <code>PoolLayer</code></p><p><strong>Arguments</strong></p><ul><li><p><code>cLayer::PoolLayer</code></p></li><li><p><code>Ai</code> := optional activation of the previous layer</p></li><li><p><code>FCache</code> := a <code>Dict</code> holds the outputs of <code>layerForProp</code> of the previous <code>Layer</code>(s)</p></li></ul><p><strong>Returns</strong></p><ul><li><code>Dict(:A =&gt; Ao)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/parallelLayerForProp.jl#L80-L102">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.layerUpdateParams!-Union{Tuple{FoB}, Tuple{Model,FoB}, Tuple{Model,FoB,Integer}} where FoB&lt;:Union{BatchNorm, FCLayer}" href="#NumNN.layerUpdateParams!-Union{Tuple{FoB}, Tuple{Model,FoB}, Tuple{Model,FoB,Integer}} where FoB&lt;:Union{BatchNorm, FCLayer}"><code>NumNN.layerUpdateParams!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">function layerUpdateParams!(
    model::Model,
    cLayer::FoB,
    cnt::Integer = -1;
    tMiniBatch::Integer = 1,
    kwargs...,
) where {FoB &lt;: Union{FCLayer, BatchNorm}}</code></pre><p>update trainable parameters for <code>FCLayer</code> and <code>BatchNorm</code> layers</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/layerUpdateParams.jl#L3-L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.oneHot-Tuple{Any}" href="#NumNN.oneHot-Tuple{Any}"><code>NumNN.oneHot</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">oneHot(Y; classes = [], numC = 0)</code></pre><p>convert array of integer classes into one Hot coding.</p><p><strong>Arguments</strong></p><ul><li><p><code>Y</code> := a vector of classes as a number</p></li><li><p><code>classes</code> := the classes explicity represented (in case not all the classes are present in the labels given)</p></li><li><p><code>numC</code> := number of classes as alternative to <code>classes</code> variable</p></li></ul><p><strong>Examples</strong></p><p>```julia Y = rand(0:9, 100); # a 100 item with class of [0-9]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/assistance.jl#L2-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.padding-Union{Tuple{T}, Tuple{AbstractArray{T,4},Integer,Integer,Integer,Integer}} where T" href="#NumNN.padding-Union{Tuple{T}, Tuple{AbstractArray{T,4},Integer,Integer,Integer,Integer}} where T"><code>NumNN.padding</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">function padding(Ai::AbstractArray{T,4},
                 p_H::Integer,
                 p_W::Integer=-1) where {T}</code></pre><p>pad zeros to the Array Ai with amount of p values</p><p>inputs:     Ai := Array of type T and dimension N     p  := integer determinde the amount of zeros padding           i.e.           if Ai is a 3-dimensional array the padding will be for the first               dimension           if Ai is a 4-dimensional array the padding will be for the first 2               dimensions           if Ai is a 5-dimensional array the padding will be for the first 3               dimensions</p><p>output:     PaddinView array where it contains the padded values and the original         data without copying it</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/padding.jl#L130-L151">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.paddingSize-Union{Tuple{PL}, Tuple{PL,Tuple}} where PL&lt;:NumNN.PaddableLayer" href="#NumNN.paddingSize-Union{Tuple{PL}, Tuple{PL,Tuple}} where PL&lt;:NumNN.PaddableLayer"><code>NumNN.paddingSize</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">function paddingSize(cLayer::PL, Ai::AbstractArray) where {PL&lt;:PaddableLayer}</code></pre><p>Helping function that returns the p<em>H</em>hi, p<em>H</em>lo, and (in case 2D Conv), p<em>W</em>hi, p<em>W</em>lo, and so on</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/padding.jl#L36-L40">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.predict" href="#NumNN.predict"><code>NumNN.predict</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">predict(model::Model, X_In::AbstractArray, Y_In = nothing; kwargs...)</code></pre><p>Run the prediction based on the trained <code>model</code></p><p><strong>Arguments</strong></p><ul><li><p><code>model::Model</code> := the trained <code>Model</code> to predict on</p></li><li><p><code>X_In</code> := the input <code>Array</code></p></li><li><p><code>Y_In</code> := labels (optional) to evaluate the model</p></li></ul><p><strong>Key-word Arugmets</strong></p><ul><li><p><code>batchSize</code> := default <code>32</code></p></li><li><p><code>useProgBar</code> := (<code>Bool</code>) where or not to shoe the prograss bar</p></li></ul><p><strong>Return</strong></p><ul><li><p>a <code>Dict</code> of:</p><ul><li><code>:YhatValue</code> := Array of the output of the integer prediction values</li><li><code>:YhatProb</code> := Array of the output probabilities</li><li><code>:accuracy</code> := the accuracy of prediction in case <code>Y_In</code> is given</li></ul></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/parallelBackForProp.jl#L135-L161">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.predictBatch" href="#NumNN.predictBatch"><code>NumNN.predictBatch</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">predictBatch(model::Model, X::AbstractArray, Y = nothing; kwargs...)</code></pre><p>predict Y using the model and the input X and the labels Y</p><p><strong>Inputs</strong></p><ul><li><p><code>model::Model</code> := the trained model</p></li><li><p><code>X::AbstractArray</code> := the input <code>Array</code></p></li><li><p><code>Y</code> := the input labels to compare with (optional)</p></li></ul><p><strong>Output</strong></p><ul><li><p>a <code>Tuple</code> of</p><ul><li><code>Ŷ</code> := the predicted values</li><li><code>Ŷ_bool</code> := the predicted labels</li><li><code>&quot;accuracy&quot;</code> := the accuracy of the predicted labels</li></ul></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/parallelBackForProp.jl#L93-L115">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.prevnextfloat-Tuple{Any}" href="#NumNN.prevnextfloat-Tuple{Any}"><code>NumNN.prevnextfloat</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">return previous float if x == 1 and nextfloat if x == 0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/lossFuns.jl#L63-L65">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.probToValue-Union{Tuple{N}, Tuple{T}, Tuple{Aa}, Tuple{Type{σ},AbstractArray{T,N}}, Tuple{Type{σ},AbstractArray{T,N},Aa}} where N where T where Aa&lt;:(Union{Nothing, #s27} where #s27&lt;:AbstractArray)" href="#NumNN.probToValue-Union{Tuple{N}, Tuple{T}, Tuple{Aa}, Tuple{Type{σ},AbstractArray{T,N}}, Tuple{Type{σ},AbstractArray{T,N},Aa}} where N where T where Aa&lt;:(Union{Nothing, #s27} where #s27&lt;:AbstractArray)"><code>NumNN.probToValue</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">function probToValue(
    actFun::Type{σ},
    probs::AbstractArray{T,N},
    labels::Aa = nothing;
    evalConst = 0.5,
) where {Aa&lt;:Union{&lt;:AbstractArray,Nothing},T,N}</code></pre><p>Convert the probabilities return out of sigmoid function to Bool value (i.e. 0,1) values based on comparing on a threshold value <code>evalConst</code></p><p><strong>Return</strong></p><ul><li><p><code>Ŷ_bool</code> := Boolean valuse of the probabilites</p></li><li><p><code>acc</code> := Accuracy when <code>labels</code> provided</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/actFuns.jl#L31-L46">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.probToValue-Union{Tuple{S}, Tuple{N}, Tuple{T}, Tuple{Type{S},AbstractArray{T,N}}} where S&lt;:NumNN.softmaxFamily where N where T" href="#NumNN.probToValue-Union{Tuple{S}, Tuple{N}, Tuple{T}, Tuple{Type{S},AbstractArray{T,N}}} where S&lt;:NumNN.softmaxFamily where N where T"><code>NumNN.probToValue</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">function probToValue(
    actFun::Type{S},
    probs::AbstractArray{T,N};
    labels = nothing,
) where {T,N,S&lt;:softmaxFamily}</code></pre><p>convert the probabilites out of <code>softmax</code> or softmax-like functions into <code>Bool</code> values, where the max value gets 1 and the other get zeros</p><p><strong>Return</strong></p><ul><li><p><code>Ŷ_bool</code> := Boolean valuse of the probabilites</p></li><li><p><code>acc</code> := Accuracy when <code>labels</code> provided</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/actFuns.jl#L131-L145">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.resetCount!-Tuple{Layer,Symbol}" href="#NumNN.resetCount!-Tuple{Layer,Symbol}"><code>NumNN.resetCount!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">resetCount!(outLayer::Layer, cnt::Symbol)</code></pre><p>to reset a counter in all layers under <code>outLayer</code>.</p><p><strong>Arguments</strong></p><ul><li><p><code>outLayer::Layer</code> := the layer from start reseting the counter</p></li><li><p><code>cnt::Symbol</code> := the counter to be reseted</p></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">X_train = rand(128, 100);

X_Input = Input(X_train);
X = FCLayer(50, :relu)(X_Input);
X_out = FCLayer(10, :softmax)(X);

FCache = chainForProp(X_train, X_Input);

# Now to reset the forwCount in all layers

resetCount!(X_out, :forwCount)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/assistance.jl#L41-L68">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.train-Tuple{Any,Any,Model,Any}" href="#NumNN.train-Tuple{Any,Any,Model,Any}"><code>NumNN.train</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">train(
      X_train,
      Y_train,
      model::Model,
      epochs;
      testData = nothing,
      testLabels = nothing,
      kwargs...,
      )</code></pre><p>Repeat the trainging (forward/backward propagation and update parameters)</p><p><strong>Argument</strong></p><ul><li><p><code>X_train</code> := the training data</p></li><li><p><code>Y_train</code> := the training labels</p></li><li><p><code>model</code>   := the model to train</p></li><li><p><code>epochs</code>  := the number of repetitions of the training phase</p></li></ul><p><strong>Key-word Arguments</strong></p><ul><li><p><code>testData</code> := to evaluate the training process over test data too</p></li><li><p><code>testLabels</code> := to evaluate the training process over test data too</p></li><li><p><code>batchSize</code> := the size of training when mini batch training</p></li></ul><p><code></code>useProgBar` := (true, false) value to use prograss bar</p><ul><li><code>kwargs</code> := other key-word Arguments to pass for the lower functions in hierarchy</li></ul><p><strong>Return</strong></p><ul><li><p>A <code>Dict{Symbol, Vector}</code> of:</p><ul><li><p><code>:trainAccuracies</code> := an <code>Array</code> of the accuracies of training data at each epoch</p></li><li><p><code>:trainCosts</code> := an <code>Array</code> of the costs of training data at each epoch</p></li><li><p>In case <code>testDate</code> and <code>testLabels</code> are givens:</p><ul><li><code>:testAccuracies</code> := an <code>Array</code> of the accuracies of test data at each epoch</li><li><code>:testCosts</code> := an <code>Array</code> of the costs of test data at each epoch</li></ul></li></ul></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/parallelBackForProp.jl#L489-L535">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.unroll" href="#NumNN.unroll"><code>NumNN.unroll</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">unroll(cLayer::Conv2D, AiS::Tuple, param::Symbol=:W)</code></pre><p>unroll the <code>param</code> of <code>Conv1D</code> into 2D matrix</p><p><strong>Arguments</strong></p><ul><li><p><code>cLayer</code> := the layer of the paramters to unroll</p></li><li><p><code>AiS</code> := the <code>padded</code> input to determinde the size and shape of the output of <code>unroll</code></p></li><li><p><code>param</code> := <code>Conv1D</code> parameter to be <code>unroll</code>ed</p></li></ul><p><strong>Return</strong></p><ul><li><code>K</code> := 2D <code>Matrix</code> of the <code>param</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/unroll.jl#L69-L85">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.unroll" href="#NumNN.unroll"><code>NumNN.unroll</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">unroll(cLayer::Conv1D, AiS::Tuple, param::Symbol=:W)</code></pre><p>unroll the <code>param</code> of <code>Conv1D</code> into 2D matrix</p><p><strong>Arguments</strong></p><ul><li><p><code>cLayer</code> := the layer of the paramters to unroll</p></li><li><p><code>AiS</code> := the <code>padded</code> input to determinde the size and shape of the output of <code>unroll</code></p></li><li><p><code>param</code> := <code>Conv1D</code> parameter to be <code>unroll</code>ed</p></li></ul><p><strong>Return</strong></p><ul><li><code>K</code> := 2D <code>Matrix</code> of the <code>param</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/unroll.jl#L8-L24">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.unroll" href="#NumNN.unroll"><code>NumNN.unroll</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">unroll(cLayer::Conv3D, AiS::Tuple, param::Symbol=:W)</code></pre><p>unroll the <code>param</code> of <code>Conv3D</code> into 2D matrix</p><p><strong>Arguments</strong></p><ul><li><p><code>cLayer</code> := the layer of the paramters to unroll</p></li><li><p><code>AiS</code> := the <code>padded</code> input to determinde the size and shape of the output of <code>unroll</code></p></li><li><p><code>param</code> := <code>Conv1D</code> parameter to be <code>unroll</code>ed</p></li></ul><p><strong>Return</strong></p><ul><li><code>K</code> := 2D <code>Matrix</code> of the <code>param</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/unroll.jl#L154-L170">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.PaddableLayer" href="#NumNN.PaddableLayer"><code>NumNN.PaddableLayer</code></a> — <span class="docstring-category">Type</span></header><section><div><p><strong>Summary</strong></p><pre><code class="language-none">abstract type PaddableLayer &lt;: Layer</code></pre><p>Abstract Type to hold all Paddable Layers (i.e.  <code>ConvLayer</code> &amp; <code>PoolLayer</code>)</p><p><strong>Subtypes</strong></p><pre><code class="language-none">ConvLayer
PoolLayer</code></pre><p><strong>Supertype Hierarchy</strong></p><pre><code class="language-none">PaddableLayer &lt;: Layer &lt;: Any</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/cnn/TypeDef.jl#L2-L17">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.getindex-Tuple{Any,Any}" href="#Base.getindex-Tuple{Any,Any}"><code>Base.getindex</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">getindex(it, key; default) = haskey(it, key) ? it[key] : default</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia">D = Dict(:A=&gt;&quot;A&quot;, :B=&gt;&quot;B&quot;)

A = getindex(D, :A)

## this will return an error
#C = getindex(D: :C)

#instead
C = getindex(D, :C; default=&quot;C&quot;)
#this will return the `String` C</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/assistance.jl#L93-L110">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.cost-Union{Tuple{N}, Tuple{T2}, Tuple{T1}, Tuple{Type{binaryCrossentropy},AbstractArray{T1,N},AbstractArray{T2,N}}} where N where T2 where T1" href="#NumNN.cost-Union{Tuple{N}, Tuple{T2}, Tuple{T1}, Tuple{Type{binaryCrossentropy},AbstractArray{T1,N},AbstractArray{T2,N}}} where N where T2 where T1"><code>NumNN.cost</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">function cost(
    loss::Type{binaryCrossentropy},
    A::AbstractArray{T1,N},
    Y::AbstractArray{T2,N},
) where {T1, T2, N}</code></pre><p>Compute the cost for <code>binaryCrossentropy</code> loss function</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/lossFuns.jl#L95-L104">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NumNN.cost-Union{Tuple{N}, Tuple{T2}, Tuple{T1}, Tuple{Type{categoricalCrossentropy},AbstractArray{T1,N},AbstractArray{T2,N}}} where N where T2 where T1" href="#NumNN.cost-Union{Tuple{N}, Tuple{T2}, Tuple{T1}, Tuple{Type{categoricalCrossentropy},AbstractArray{T1,N},AbstractArray{T2,N}}} where N where T2 where T1"><code>NumNN.cost</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">function cost(
    loss::Type{categoricalCrossentropy},
    A::AbstractArray{T1,N},
    Y::AbstractArray{T2,N},
) where {T1, T2, N}</code></pre><p>Compute the cost for <code>categoricalCrossentropy</code> loss function</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MohHizzani/NumNN.jl/blob/01d513255e283ff372903372748b92f405732cf9/src/lossFuns.jl#L73-L82">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../tutorials/">Tutorials »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 17 April 2020 22:23">Friday 17 April 2020</span>. Using Julia version 1.4.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
