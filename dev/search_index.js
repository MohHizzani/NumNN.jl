var documenterSearchIndex = {"docs":
[{"location":"tutorials/#Tutorials-1","page":"Tutorials","title":"Tutorials","text":"","category":"section"},{"location":"tutorials/#","page":"Tutorials","title":"Tutorials","text":"To learn how to use NumNN, here some tutorials as Jupyter notebooks","category":"page"},{"location":"tutorials/#[Intro-to-**NumNN**-using-Fully-Connected-Layers](https://nbviewer.jupyter.org/github/MohHizzani/NumNN.jl/blob/master/examples/00_FCLayer_FashionMNIST.ipynb)-1","page":"Tutorials","title":"Intro to NumNN using Fully-Connected Layers","text":"","category":"section"},{"location":"tutorials/#[Intro-to-**NumNN**-using-Convolution-Neural-Networks](https://nbviewer.jupyter.org/github/MohHizzani/NumNN.jl/blob/master/examples/01_CNN_FashionMNIST.ipynb)-1","page":"Tutorials","title":"Intro to NumNN using Convolution Neural Networks","text":"","category":"section"},{"location":"tutorials/#[Example-of-**Inception**-Neural-Network](https://nbviewer.jupyter.org/github/MohHizzani/NumNN.jl/blob/master/examples/02_CNN_Inception_FashionMNIST.ipynb)-1","page":"Tutorials","title":"Example of Inception Neural Network","text":"","category":"section"},{"location":"#NumNN.jl-1","page":"Home","title":"NumNN.jl","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"This package provides high-level Neural Network APIs deals with different number representations like Posit, Logarithmic Data Representations, Residual Number System (RNS), and -for sure- the conventional IEEE formats.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Since, the implementation and development process for testing novel number systems on different Deep Learning applications using the current available DP frameworks in easily feasible. An urgent need for an unconventional library that provides both the easiness and complexity of simulating and testing and evaluate new number systems before the hardware design complex process— was resurfaced.","category":"page"},{"location":"#Why-Julia?-1","page":"Home","title":"Why Julia?","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Julia provides in an unconventional way the ability to simulate new number systems and deploy this simulation to be used as high-level primitive type. Multiple Dispatch provides a unique ability to write a general code then specify the implementation based on the type.","category":"page"},{"location":"#Examples-of-Multiple-Dispatch-1","page":"Home","title":"Examples of Multiple Dispatch","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"julia> aInt = 1; #with Integer type\n\njulia> bInt = 2; #with Integer type\n\njulia> cInt = 1 + 2; #which is a shortcut for cInt = +(1,2)\n\njulia> aFloat = 1.0; #with Float64 type\n\njulia> bFloat = 2.0; #with Flot64 type\n\njulia> aInt + bFloat #will use the method +(::Int64, ::Float64)\n3.0","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Now let's do something more interesting with Posit (continue on the previous example)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"julia> using SoftPosit\n\njulia> aP = Posit16(1)\nPosit16(0x4000)\n\njulia> bP = Posit16(2)\nPosit16(0x5000)\n\njulia> aInt + aP #Note how the result is in Posit16 type\nPosit16(0x5000)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The output was of type Posit16 because in Julia you can define a Promote Rule which mean when the output can be in either type(s) of the input, promote the output to be of the specified type. Which can be defined as follows:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Base.promote_rule(::Type{Int64}, ::Type{Posit16}) = Posit16","category":"page"},{"location":"#","page":"Home","title":"Home","text":"This means that the output of an operation on both Int64 and Posit16 should be converted to Posit16.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Pages = [\"docstrings.md\", \"tutorials.md\"]","category":"page"},{"location":"docstrings/#Docstrings-1","page":"Docstrings","title":"Docstrings","text":"","category":"section"},{"location":"docstrings/#","page":"Docstrings","title":"Docstrings","text":"Modules = [NumNN]\nOrder   = [:type, :function]","category":"page"},{"location":"docstrings/#NumNN.Activation","page":"Docstrings","title":"NumNN.Activation","text":"Activation(actFun)\n\nArguments\n\nactFun::Symbol := the activation function of this layer\n\n\n\nSummary\n\nmutable struct Activation <: Layer\n\n\n\nFields\n\nactFun::Symbol := the activation function of this layer\nchannels::Integer := is the number of nodes or channels in the layer\ninputS::Tuple := input size of the layer\noutputS::Tuple := output size of the layer\nforwCount::Integer := forward propagation counter\nbackCount::Integer := backward propagation counter\nupdateCount::Integer := update parameters counter\nnextLayers::Array{Layer,1} := An array of the next layer(s)\nprevLayer::Array{Layer,1} := An array of the previous layer(s) to be added\n\n\n\nSupertype Hierarchy\n\nActivation <: Layer <: An\n\n\n\nExamples\n\nX_Input = Input(X_train)\nX = FCLayer(10, :noAct)(X_Input)\nX = Activation(:relu)(X)\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#NumNN.AddLayer","page":"Docstrings","title":"NumNN.AddLayer","text":"AddLayer(; [channels = 0])\n\nLayer performs and addition of multiple previous layers\n\nArguments\n\nchannels := (Integer) number of channels/nodes of this array which equals to the same of the previous layer(s)\n\n\n\nSummary\n\nmutable struct AddLayer <: MILayer\n\nFields\n\nchannels::Integer := is the number of nodes or channels in the layer\ninputS::Tuple := input size of the layer\noutputS::Tuple := output size of the layer\nforwCount::Integer := forward propagation counter\nbackCount::Integer := backward propagation counter\nupdateCount::Integer := update parameters counter\nnextLayers::Array{Layer,1} := An array of the next layer(s)\nprevLayer::Array{Layer,1} := An array of the previous layer(s) to be added\n\n\n\nSupertype Hierarchy\n\nAddLayer <: MILayer <: Layer <: Any\n\n\n\nExamples\n\nXIn1 = Input(X_train)\nX1 = FCLayer(10, :relu)(XIn1)\nXIn2 = Input(X_train)\nX2 = FCLayer(10, :tanh)(XIn2)\n\nXa = AddLayer()([X1,X2])\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#NumNN.AveragePool1D","page":"Docstrings","title":"NumNN.AveragePool1D","text":"AveragePool1D(\n    f::Integer=2;\n    prevLayer=nothing,\n    strides::Integer=f,\n    padding::Symbol=:valid,\n)\n\nSummary\n\nmutable struct AveragePool1D <: AveragePoolLayer\n\nFields\n\nchannels    :: Integer\nf           :: Integer\ns           :: Integer\ninputS      :: Tuple\noutputS     :: Tuple\npadding     :: Symbol\nforwCount   :: Integer\nbackCount   :: Integer\nupdateCount :: Integer\nprevLayer   :: Union{Nothing, Layer}\nnextLayers  :: Array{Layer,1}\n\nSupertype Hierarchy\n\nAveragePool1D <: AveragePoolLayer <: PoolLayer <: PaddableLayer <: Layer <: Any\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#NumNN.AveragePool2D","page":"Docstrings","title":"NumNN.AveragePool2D","text":"AveragePool2D(\n    f::Tuple{Integer,Integer}=(2,2);\n    prevLayer=nothing,\n    strides::Tuple{Integer,Integer}=f,\n    padding::Symbol=:valid,\n)\n\nSummary\n\nmutable struct AveragePool2D <: AveragePoolLayer\n\nFields\n\nchannels    :: Integer\nf           :: Tuple{Integer,Integer}\ns           :: Tuple{Integer,Integer}\ninputS      :: Tuple\noutputS     :: Tuple\npadding     :: Symbol\nforwCount   :: Integer\nbackCount   :: Integer\nupdateCount :: Integer\nprevLayer   :: Union{Nothing, Layer}\nnextLayers  :: Array{Layer,1}\n\nSupertype Hierarchy\n\nAveragePool2D <: AveragePoolLayer <: PoolLayer <: PaddableLayer <: Layer <: Any\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#NumNN.AveragePool3D","page":"Docstrings","title":"NumNN.AveragePool3D","text":"AveragePool3D(\n    f::Tuple{Integer,Integer,Integer}=(2,2,2);\n    prevLayer=nothing,\n    strides::Tuple{Integer,Integer,Integer}=f,\n    padding::Symbol=:valid,\n)\n\nSummary\n\nmutable struct AveragePool3D <: AveragePoolLayer\n\nFields\n\nchannels    :: Integer\nf           :: Tuple{Integer,Integer,Integer}\ns           :: Tuple{Integer,Integer,Integer}\ninputS      :: Tuple\noutputS     :: Tuple\npadding     :: Symbol\nforwCount   :: Integer\nbackCount   :: Integer\nupdateCount :: Integer\nprevLayer   :: Union{Nothing, Layer}\nnextLayers  :: Array{Layer,1}\n\nSupertype Hierarchy\n\nAveragePool3D <: AveragePoolLayer <: PoolLayer <: PaddableLayer <: Layer <: Any\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#NumNN.AveragePoolLayer","page":"Docstrings","title":"NumNN.AveragePoolLayer","text":"Summary\n\nabstract type AveragePoolLayer <: PoolLayer\n\nSubtypes\n\nAveragePool1D\nAveragePool2D\nAveragePool3D\n\nSupertype Hierarchy\n\nAveragePoolLayer <: PoolLayer <: PaddableLayer <: Layer <: Any\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#NumNN.BatchNorm","page":"Docstrings","title":"NumNN.BatchNorm","text":"BatchNorm(;dim=1, ϵ=1e-10)\n\nBatch Normalization Layer that is used ot normalize across the dimensions specified by the argument dim.\n\nArguments\n\ndim::Integer := is the dimension to normalize across\nϵ::AbstractFloat := is a backup constant that is used to prevent from division on zero when σ^2 is zero\n\n\n\nSummary\n\nmutable struct BatchNorm <: Layer\n\n\n\nFields\n\nchannels::Integer := is the number of nodes in the layer\ninputS::Tuple{Integer, Integer} := input size of the layer, of the shape (channels of the previous layer, size of mini-batch)\noutputS::Tuple{Integer, Integer} := output size of the layer, of the shape (channels of this layer, size of mini-batch)\ndim::Integer := the dimension to normalize across\nϵ::AbstractFloat := backup constant to protect from dividing on zero when σ^2 = 0\nW::Array{T,2} where {T} := the scaling parameters of this layer W * X, same shape of the mean μ\nB::Array{T,2} where {T} := the bias of this layer W * X .+ B, same shape of the variance σ^2\ndW::Array{T,2} where {T} := the derivative of the loss function to the W parameters fracdJdW\ndB::Array{T,2} where {T} := the derivative of the loss function to the B parameters fracdJdB\nforwCount::Integer := forward propagation counter\nbackCount::Integer := backward propagation counter\nupdateCount::Integer := update parameters counter\nprevLayer::L where {L<:Union{Layer,Nothing}} := the previous layer which is\n\nthe input of this layer\n\nnextLayers::Array{Layer,1} := An array of the next layer(s)\n\n\n\nSupertype Hierarchy\n\nBatchNorm <: Layer <: Any\n\n\n\nExamples\n\nX_train = rand(14,14,3,32) #input of shape `14×14` with channels of `3` and mini-batch size `32`\n\nX_Input = Input(X_train)\nX = Conv2D(10, (3,3))(X_Input)\nX = BatchNorm(dim=3) #to normalize across the channels dimension\nX = Activation(:relu)\n\nX_train = rand(128,5,32) #input of shape `128` with channels of `5` and mini-batch size `32`\n\nX_Input = Input(X_train)\nX = Conv1D(10, 5)(X_Input)\nX = BatchNorm(dim=2) #to normalize across the channels dimension\nX = Activation(:relu)\n``\n\n\n\njulia X_train = rand(6464,32) #input of shape `6464and mini-batch size32`\n\nXInput = Input(Xtrain) X = FCLayer(10, :noAct)(X_Input) X = BatchNorm(dim=1) #to normalize across the features dimension X = Activation(:relu) ````\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#NumNN.ConcatLayer","page":"Docstrings","title":"NumNN.ConcatLayer","text":"ConcatLayer(; channels = 0)\n\nPerform concatenation of group of previous Layers\n\nSummary\n\nmutable struct ConcatLayer <: MILayer\n\nFields\n\nchannels    :: Integer\ninputS      :: Tuple\noutputS     :: Tuple\nforwCount   :: Integer\nbackCount   :: Integer\nupdateCount :: Integer\nnextLayers  :: Array{Layer,1}\nprevLayer   :: Array{Layer,1}\n\nSupertype Hierarchy\n\nConcatLayer <: MILayer <: Layer <: Any\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#NumNN.Conv1D","page":"Docstrings","title":"NumNN.Conv1D","text":"Summary\n\nmutable struct Conv1D <: ConvLayer\n\nFields\n\nchannels    :: Integer\nf           :: Integer\ns           :: Integer\ninputS      :: Tuple\noutputS     :: Tuple\npadding     :: Symbol\nW           :: Array{F,3} where F\ndW          :: Array{F,3} where F\nK           :: Array{F,2} where F\ndK          :: Array{F,2} where F\nB           :: Array{F,3} where F\ndB          :: Array{F,3} where F\nactFun      :: Symbol\nkeepProb    :: AbstractFloat\nV           :: Dict{Symbol,Array{F,3} where F}\nS           :: Dict{Symbol,Array{F,3} where F}\nV̂dk         :: Array{F,2} where F\nŜdk         :: Array{F,2} where F\nforwCount   :: Integer\nbackCount   :: Integer\nupdateCount :: Integer\nprevLayer   :: Union{Nothing, Layer}\nnextLayers  :: Array{Layer,1}\n\nSupertype Hierarchy\n\nConv1D <: ConvLayer <: PaddableLayer <: Layer <: Any\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#NumNN.Conv2D","page":"Docstrings","title":"NumNN.Conv2D","text":"Summary\n\nmutable struct Conv2D <: ConvLayer\n\nFields\n\nchannels    :: Integer\nf           :: Tuple{Integer,Integer}\ns           :: Tuple{Integer,Integer}\ninputS      :: Tuple\noutputS     :: Tuple\npadding     :: Symbol\nW           :: Array{F,4} where F\ndW          :: Array{F,4} where F\nK           :: Array{F,2} where F\ndK          :: Array{F,2} where F\nB           :: Array{F,4} where F\ndB          :: Array{F,4} where F\nactFun      :: Symbol\nkeepProb    :: AbstractFloat\nV           :: Dict{Symbol,Array{F,4} where F}\nS           :: Dict{Symbol,Array{F,4} where F}\nV̂dk         :: Array{F,2} where F\nŜdk         :: Array{F,2} where F\nforwCount   :: Integer\nbackCount   :: Integer\nupdateCount :: Integer\nprevLayer   :: Union{Nothing, Layer}\nnextLayers  :: Array{Layer,1}\n\nSupertype Hierarchy\n\nConv2D <: ConvLayer <: PaddableLayer <: Layer <: Any\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#NumNN.Conv3D","page":"Docstrings","title":"NumNN.Conv3D","text":"Summary\n\nmutable struct Conv3D <: ConvLayer\n\nFields\n\nchannels    :: Integer\nf           :: Tuple{Integer,Integer,Integer}\ns           :: Tuple{Integer,Integer,Integer}\ninputS      :: Tuple\noutputS     :: Tuple\npadding     :: Symbol\nW           :: Array{F,5} where F\ndW          :: Array{F,5} where F\nK           :: Array{F,2} where F\ndK          :: Array{F,2} where F\nB           :: Array{F,5} where F\ndB          :: Array{F,5} where F\nactFun      :: Symbol\nkeepProb    :: AbstractFloat\nV           :: Dict{Symbol,Array{F,5} where F}\nS           :: Dict{Symbol,Array{F,5} where F}\nV̂dk        :: Array{F,2} where F\nŜdk         :: Array{F,2} where F\nforwCount   :: Integer\nbackCount   :: Integer\nupdateCount :: Integer\nprevLayer   :: Union{Nothing, Layer}\nnextLayers  :: Array{Layer,1}\n\nSupertype Hierarchy\n\nConv3D <: ConvLayer <: PaddableLayer <: Layer <: Any\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#NumNN.ConvLayer","page":"Docstrings","title":"NumNN.ConvLayer","text":"Summary\n\nabstract type ConvLayer <: PaddableLayer\n\nAbstract Type to hold all ConvLayer\n\nSubtypes\n\nConv1D\nConv2D\nConv3D\n\nSupertype Hierarchy\n\nConvLayer <: PaddableLayer <: Layer <: Any\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#NumNN.FCLayer","page":"Docstrings","title":"NumNN.FCLayer","text":"FCLayer(channels=0, actFun=:noAct, [layerInput = nothing; keepProb = 1.0])\n\nFully-connected layer (equivalent to Dense in TensorFlow etc.)\n\nArguments\n\nchannels := (Integer) is the number of nodes in the layer\nactFun := (Symbol) is the activation function of this layer\nlayerInput := (Layer or Array) the input of this array (optional don't need to assign it)\nkeepProb := (AbstractFloat) the keep probability (1 - prob of the dropout rate)\n\n\n\nSummary\n\nmutable struct FCLayer <: Layer\n\n\n\nFields\n\nchannels::Integer := is the number of nodes in the layer\nactFun::Symbol := the activation function of this layer\ninputS::Tuple{Integer, Integer} := input size of the layer, of the shape (channels of the previous layer, size of mini-batch)\noutputS::Tuple{Integer, Integer} := output size of the layer, of the shape (channels of this layer, size of mini-batch)\nkeepProb::AbstractFloat := the keep probability (rate) of the drop-out operation <1.0\nW::Array{T,2} where {T} := the scaling parameters of this layer W * X, of the shape (channels of this layer, channels of the previous layer)\nB::Array{T,2} where {T} := the bias of this layer W * X .+ B, of the shape (channels of this layer, 1)\ndW::Array{T,2} where {T} := the derivative of the loss function to the W parameters fracdJdW\ndB::Array{T,2} where {T} := the derivative of the loss function to the B parameters fracdJdB\nforwCount::Integer := forward propagation counter\nbackCount::Integer := backward propagation counter\nupdateCount::Integer := update parameters counter\nprevLayer::L where {L<:Union{Layer,Nothing}} := the previous layer which is\n\nthe input of this layer\n\nnextLayers::Array{Layer,1} := An array of the next layer(s)\n\n\n\nSupertype Hierarchy\n\nFCLayer <: Layer <: Any\n\n\n\nExamples\n\nX_Input = Input(X_train)\nX = FCLayer(20, :relu)(X_Input)\n\n\nIn the previous example the variable X_Input is a pointer to the Input layer, and X is an pointer to the FCLayer(20, :relu) layer. Note that the layer instance can be used as a connecting function.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#NumNN.FCLayer-Tuple{Array}","page":"Docstrings","title":"NumNN.FCLayer","text":"define input as X\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.FCLayer-Tuple{Layer}","page":"Docstrings","title":"NumNN.FCLayer","text":"connect with the previous layer\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.Flatten","page":"Docstrings","title":"NumNN.Flatten","text":"Flatten()\n\nFlatten the input into 2D Array\n\nSummary\n\nmutable struct Flatten <: Layer\n\nFields\n\nchannels    :: Integer\ninputS      :: Tuple\noutputS     :: Tuple\nforwCount   :: Integer\nbackCount   :: Integer\nupdateCount :: Integer\nnextLayers  :: Array{Layer,1}\nprevLayer   :: Union{Nothing, Layer}\n\nSupertype Hierarchy\n\nFlatten <: Layer <: Any\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#NumNN.Input","page":"Docstrings","title":"NumNN.Input","text":"Input(X_shape::Tuple)\n\nInput Layer that is used as a pointer to the input array(s).\n\nArguments\n\nX_shape::Tuple := shape of the input Array\n\n\n\nSummary\n\nmutable struct Input <: Layer\n\nFields\n\nchannels::Integer := is the number of nodes or channels in the layer\ninputS::Tuple := input size of the layer\noutputS::Tuple := output size of the layer\nforwCount::Integer := forward propagation counter\nbackCount::Integer := backward propagation counter\nupdateCount::Integer := update parameters counter\nnextLayers::Array{Layer,1} := An array of the next layer(s)\nprevLayer::Array{Layer,1} := An array of the previous layer(s) to be added\n\n\n\nSupertype Hierarchy\n\nInput <: Layer <: Any\n\n\n\nExamples\n\nX_Input = Input(size(X_train))\nX = FCLayer(10, :relu)(X_Input)\n\nIt is possible to use the Array instead of its size NumNN will take care of the rest\n\nX_Input = Input(X_train)\nX = FCLayer(10, :relu)(X_Input)\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#NumNN.Layer","page":"Docstrings","title":"NumNN.Layer","text":"abstract type to include all layers\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#NumNN.MaxPool1D","page":"Docstrings","title":"NumNN.MaxPool1D","text":"MaxPool1D(\n    f::Integer=2;\n    prevLayer=nothing,\n    strides::Integer=f,\n    padding::Symbol=:valid,\n)\n\nSummary\n\nmutable struct MaxPool1D <: MaxPoolLayer\n\nFields\n\nchannels    :: Integer\nf           :: Integer\ns           :: Integer\ninputS      :: Tuple\noutputS     :: Tuple\npadding     :: Symbol\nforwCount   :: Integer\nbackCount   :: Integer\nupdateCount :: Integer\nprevLayer   :: Union{Nothing, Layer}\nnextLayers  :: Array{Layer,1}\n\nSupertype Hierarchy\n\nMaxPool1D <: MaxPoolLayer <: PoolLayer <: PaddableLayer <: Layer <: Any\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#NumNN.MaxPool2D","page":"Docstrings","title":"NumNN.MaxPool2D","text":"MaxPool2D(\n    f::Tuple{Integer,Integer}=(2,2);\n    prevLayer=nothing,\n    strides::Tuple{Integer,Integer}=f,\n    padding::Symbol=:valid,\n)\n\nSummary\n\nmutable struct MaxPool2D <: MaxPoolLayer\n\nFields\n\nchannels    :: Integer\nf           :: Tuple{Integer,Integer}\ns           :: Tuple{Integer,Integer}\ninputS      :: Tuple\noutputS     :: Tuple\npadding     :: Symbol\nforwCount   :: Integer\nbackCount   :: Integer\nupdateCount :: Integer\nprevLayer   :: Union{Nothing, Layer}\nnextLayers  :: Array{Layer,1}\n\nSupertype Hierarchy\n\nMaxPool2D <: MaxPoolLayer <: PoolLayer <: PaddableLayer <: Layer <: Any\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#NumNN.MaxPool3D","page":"Docstrings","title":"NumNN.MaxPool3D","text":"MaxPool3D(\n    f::Tuple{Integer,Integer,Integer}=(2,2,2);\n    prevLayer=nothing,\n    strides::Tuple{Integer,Integer,Integer}=f,\n    padding::Symbol=:valid,\n)\n\nSummary\n\nmutable struct MaxPool3D <: MaxPoolLayer\n\nFields\n\nchannels    :: Integer\nf           :: Tuple{Integer,Integer,Integer}\ns           :: Tuple{Integer,Integer,Integer}\ninputS      :: Tuple\noutputS     :: Tuple\npadding     :: Symbol\nforwCount   :: Integer\nbackCount   :: Integer\nupdateCount :: Integer\nprevLayer   :: Union{Nothing, Layer}\nnextLayers  :: Array{Layer,1}\n\nSupertype Hierarchy\n\nMaxPool3D <: MaxPoolLayer <: PoolLayer <: PaddableLayer <: Layer <: Any\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#NumNN.MaxPoolLayer","page":"Docstrings","title":"NumNN.MaxPoolLayer","text":"Summary\n\nabstract type MaxPoolLayer <: PoolLayer\n\nAbstract Type to hold all the MaxPoolLayers\n\nSubtypes\n\nMaxPool1D\nMaxPool2D\nMaxPool3D\n\nSupertype Hierarchy\n\nMaxPoolLayer <: PoolLayer <: PaddableLayer <: Layer <: Any\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#NumNN.Model","page":"Docstrings","title":"NumNN.Model","text":"function Model(\n    X,\n    Y,\n    inLayer::Layer,\n    outLayer::Layer,\n    α;\n    optimizer = :gds,\n    β1 = 0.9,\n    β2 = 0.999,\n    ϵAdam = 1e-8,\n    regulization = 0,\n    λ = 1.0,\n    lossFun = :categoricalCrossentropy,\n    paramsDtype::DataType = Float64,\n)\n\nSummary\n\nmutable struct Model <: Any\n\nFields\n\ninLayer      :: Layer\noutLayer     :: Layer\nlossFun      :: Symbol\nparamsDtype  :: DataType\nregulization :: Integer\nλ            :: AbstractFloat\nα            :: AbstractFloat\noptimizer    :: Symbol\nϵAdam        :: AbstractFloat\nβ1           :: AbstractFloat\nβ2           :: AbstractFloat\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#NumNN.PoolLayer","page":"Docstrings","title":"NumNN.PoolLayer","text":"Summary\n\nabstract type PoolLayer <: PaddableLayer\n\nAbstract Type to hold all the PoolLayers\n\nSubtypes\n\nAveragePoolLayer\nMaxPoolLayer\n\nSupertype Hierarchy\n\nPoolLayer <: PaddableLayer <: Layer <: Any\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#NumNN.binaryCrossentropy","page":"Docstrings","title":"NumNN.binaryCrossentropy","text":"return the average cross entropy loss over vector of labels and predictions\n\ninput:\n    a := (?1, c,m) matrix of predicted values, where c is the number of classes\n    y := (?1, c,m) matrix of predicted values, where c is the number of classes\n\n    Note: in case the number of classes is one (1) it is okay to have\n          a scaler values for a and y\n\noutput:\n    J := scaler value of the cross entropy loss\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#NumNN.relu-Union{Tuple{AbstractArray{T,N}}, Tuple{N}, Tuple{T}} where N where T","page":"Docstrings","title":"NumNN.relu","text":"return the ReLU output\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.softmax-Union{Tuple{AbstractArray{T,N}}, Tuple{N}, Tuple{T}} where N where T","page":"Docstrings","title":"NumNN.softmax","text":"compute the softmax function\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.σ-Tuple{Any,Any,Any}","page":"Docstrings","title":"NumNN.σ","text":"return the Sigmoid output\ninputs must be matices\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.NNConv-Union{Tuple{CL}, Tuple{N}, Tuple{T}, Tuple{CL,AbstractArray{T,N}}} where CL<:ConvLayer where N where T","page":"Docstrings","title":"NumNN.NNConv","text":"NNConv(cLayer::CL, Ai::AbstractArray{T,N}) where {T,N, CL <: ConvLayer}\n\nPerform the forward propagation for cLayer::ConvLayer using fast implementation of NNlib\n\nReturn\n\nDict(:Z => Z, :A => A)\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.chain-Union{Tuple{L}, Tuple{Any,Array{L,1}}} where L<:Layer","page":"Docstrings","title":"NumNN.chain","text":"function chain(X, arr::Array{L,1}) where {L<:Layer}\n\nReturns the input Layer and the output Layer from an Array of layers and the input of the model as and Array X\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.chainBackProp-Union{Tuple{N2}, Tuple{N1}, Tuple{T2}, Tuple{T1}, Tuple{L}, Tuple{AbstractArray{T1,N1},AbstractArray{T2,N2},Model,Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{AbstractArray{T1,N1},AbstractArray{T2,N2},Model,Dict{Layer,Dict{Symbol,AbstractArray}},L}, Tuple{AbstractArray{T1,N1},AbstractArray{T2,N2},Model,Dict{Layer,Dict{Symbol,AbstractArray}},L,Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{AbstractArray{T1,N1},AbstractArray{T2,N2},Model,Dict{Layer,Dict{Symbol,AbstractArray}},L,Dict{Layer,Dict{Symbol,AbstractArray}},Any}} where N2 where N1 where T2 where T1 where L<:Union{Nothing, Layer}","page":"Docstrings","title":"NumNN.chainBackProp","text":"function chainBackProp(\n    X::AbstractArray{T1,N1},\n    Y::AbstractArray{T2,N2},\n    model::Model,\n    FCache::Dict{Layer,Dict{Symbol,AbstractArray}},\n    cLayer::L = nothing,\n    BCache::Dict{Layer,Dict{Symbol,AbstractArray}}=Dict{Layer,Dict{Symbol,AbstractArray}}(),\n    cnt = -1;\n    tMiniBatch::Integer = -1, #can be used to perform both back and update params\n    kwargs...,\n) where {L<:Union{Layer,Nothing},T1,T2,N1,N2}\n\nArguments\n\nX := train data\nY := train labels\nmodel := is the model to perform the back propagation on\nFCache := the cached values of the forward propagation as Dict{Layer, Dict{Symbol, AbstractArray}}\ncLayer := is an internal variable to hold the current layer\nBCache := to hold the cache of the back propagtion (internal variable)\ncnt := is an internal variable to count the step of back propagation currently on to avoid re-do it\n\nKey-word Arguments\n\ntMiniBatch := to perform both the back prop and update trainable parameters in the same recursive call (if less than 1 update during back propagation is ditched)\nkwargs := other key-word arguments to be bassed to layerBackProp methods\n\nReturn\n\nBCache := the cached values of the back propagation\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.chainForProp-Union{Tuple{N}, Tuple{T}, Tuple{AbstractArray{T,N},Layer}, Tuple{AbstractArray{T,N},Layer,Integer}} where N where T","page":"Docstrings","title":"NumNN.chainForProp","text":"function chainForProp(\n    X::AbstractArray{T,N},\n    cLayer::Layer,\n    cnt::Integer = -1;\n    FCache = Dict{Layer,Dict{Symbol,AbstractArray}}(),\n    kwargs...,\n) where {T,N}\n\nperform the chained forward propagation using recursive calls\n\nArguments:\n\nX::AbstractArray{T,N} := input of the input layer\ncLayer::Layer := Input Layer\ncnt::Integer := an internal counter used to cache the layers was performed not to redo it again\n\nReturns\n\nCache::Dict{Layer, Dict{Symbol, Array}} := the output each layer either A, Z or together As Dict of layer to dict of Symbols and Arrays for internal use, it set again the values of Z and A in each layer to be used later in back propagation and add one to the layer forwCount value when pass through it\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.chainUpdateParams!-Union{Tuple{Model}, Tuple{L}, Tuple{Model,L}, Tuple{Model,L,Any}} where L<:Union{Nothing, Layer}","page":"Docstrings","title":"NumNN.chainUpdateParams!","text":"chainUpdateParams!(model::Model,\n                   cLayer::L=nothing,\n                   cnt = -1;\n                   tMiniBatch::Integer = 1) where {L<:Union{Layer,Nothing}}\n\nUpdate trainable parameters using recursive call\n\nArguments\n\nmodel := the model holds the training and update process\ncLayer := internal variable for recursive call holds the current layer\ncnt := an internal variable to hold the count of update in each layer not to re-do it\n\nKey-word Arguments\n\ntMiniBatch := the number of mini-batch of the total train collection\n\nReturn\n\nnothing\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.dNNConv!-Union{Tuple{CL}, Tuple{N}, Tuple{T3}, Tuple{T2}, Tuple{T1}, Tuple{CL,AbstractArray{T1,N},AbstractArray{T2,N},AbstractArray{T3,N}}} where CL<:ConvLayer where N where T3 where T2 where T1","page":"Docstrings","title":"NumNN.dNNConv!","text":"function dNNConv!(\n    cLayer::CL,\n    Ai::AbstractArray{T1,N},\n    dAi::AbstractArray{T2,N},\n    dZ::AbstractArray{T3,N},\n) where {T1, T2, T3, N, CL <: ConvLayer}\n\nPerforms the back propagation for cLayer::ConvLayer and save values to the pre-allocated Array dAi and trainable parameters W & B\n\nArguments\n\ncLayer::ConvLayer\nAi::AbstractArray{T1,N} := the input activation of cLayer\ndAi::AbstractArray{T2,N} := pre-allocated to hold the derivative of the activation\ndZ::AbstractArray{T3,N} := the derivative of the cost to the input of the activation function\n\nReturn\n\nnothing\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.dbinaryCrossentropy-Tuple{Any,Any}","page":"Docstrings","title":"NumNN.dbinaryCrossentropy","text":"compute the drivative of cross-entropy loss function to the input of the\nlayer dZ\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.deepInitWB!","page":"Docstrings","title":"NumNN.deepInitWB!","text":"initialize W's and B's using\n\ninputs:\n    X := is the input of the neural Network\n    outLayer := is the output Layer or the current layer\n                of initialization\n    cnt := is a counter to determinde the current step\n            and its an internal variable\n\n\n    kwargs:\n        He := is a true/false array, whether to use the He **et al.** initialization\n                or not\n\n        coef := when not using He **et al.** initialization use this coef\n                to multiply with the random numbers initialization\n        zro := true/false variable whether to initialize W with zeros or not\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#NumNN.drelu-Union{Tuple{AbstractArray{T,N}}, Tuple{N}, Tuple{T}} where N where T","page":"Docstrings","title":"NumNN.drelu","text":"return the derivative of ReLU function\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.dσ-Tuple{Any}","page":"Docstrings","title":"NumNN.dσ","text":"return the derivative of Sigmoid function\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.getLayerSlice-Tuple{Layer,Layer,Dict{Layer,Dict{Symbol,AbstractArray}}}","page":"Docstrings","title":"NumNN.getLayerSlice","text":"getLayerSlice(cLayer::Layer, nextLayer::Layer, BCache::Dict{Layer, Dict{Symbol, AbstractArray}})\n\nFall back method for  Layers other than ConcatLayer\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.initWB!-Union{Tuple{FCLayer}, Tuple{T}, Tuple{FCLayer,Type{T}}} where T","page":"Docstrings","title":"NumNN.initWB!","text":"initialize W and B for layer with inputs of size of (nl_1) and layer size\n    of (nl)\n\nreturns:\n    W: of size of (nl, nl_1)\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.layerBackProp","page":"Docstrings","title":"NumNN.layerBackProp","text":"layerBackProp(\n    cLayer::BatchNorm,\n    model::Model,\n    FCache::Dict{Layer, Dict{Symbol, AbstractArray}},\n    BCache::Dict{Layer, Dict{Symbol, AbstractArray}},\n    dAo::AbstractArray = Array{Any,1}(undef,0);\n    labels::AbstractArray = Array{Any,1}(undef,0),\n    kwargs...,\n)\n\nPerform the back propagation of BatchNorm type on the activations and trainable parameters W and B\n\nArgument\n\ncLayer := the layer to perform the backprop on\nmodel := the Model\nFCache := the cache values of the forprop\nBCache := the cache values of the backprop from the front Layer(s)\ndAo := (for test purpose) the derivative of the front Layer\nlabels := in case this is the output layer\n\nReturn\n\nA Dict{Symbol, AbstractArray}(:dA => dAi)\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#NumNN.layerBackProp-Union{Tuple{CL}, Tuple{CL,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{CL,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray}, Tuple{CL,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray,AbstractArray}, Tuple{CL,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray,AbstractArray,AbstractArray}} where CL<:ConvLayer","page":"Docstrings","title":"NumNN.layerBackProp","text":"function layerBackProp(\n    cLayer::ConvLayer,\n    model::Model,\n    FCache::Dict{Layer, Dict{Symbol, AbstractArray}},\n    BCache::Dict{Layer, Dict{Symbol, AbstractArray}},\n    Ai::AbstractArray = Array{Any,1}(undef,0),\n    Ao::AbstractArray = Array{Any,1}(undef,0),\n    dAo::AbstractArray = Array{Any,1}(undef,0);\n    labels::AbstractArray = Array{Any,1}(undef,0),\n    kwargs...\n)\n\nPerforms the layer back propagation for a ConvLayer\n\nArguments\n\ncLayer::ConvLayer\nmodel::Model\nFCache := the cache of the forward propagation step\nBCache := the cache of so far done back propagation\nfor test purpose   Ai   Ao   dAo\nlabels := when cLayer is an output Layer\n\nReturn\n\nDict(:dA => dAi)\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.layerBackProp-Union{Tuple{N}, Tuple{T2}, Tuple{T1}, Tuple{Activation,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{Activation,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray{T1,N}}} where N where T2 where T1","page":"Docstrings","title":"NumNN.layerBackProp","text":"layerBackProp(\n    cLayer::Activation,\n    model::Model,\n    FCache::Dict{Layer, Dict{Symbol, AbstractArray}},\n    BCache::Dict{Layer, Dict{Symbol, AbstractArray}},\n    dAo::AbstractArray{T1,N} = Array{Any,1}(undef,0);\n    labels::AbstractArray{T2,N} = Array{Any,1}(undef,0),\n    kwargs...,\n) where {T1,T2,N}\n\nPerform the back propagation of Activation type\n\nArgument\n\ncLayer := the layer to perform the backprop on\nmodel := the Model\nFCache := the cache values of the forprop\nBCache := the cache values of the backprop from the front Layer(s)\ndAo := (for test purpose) the derivative of the front Layer\nlabels := in case this is the output layer\n\nReturn\n\nA Dict{Symbol, AbstractArray}(:dA => dAi)\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.layerBackProp-Union{Tuple{N}, Tuple{T2}, Tuple{T1}, Tuple{AddLayer,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{AddLayer,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray{T1,N}}} where N where T2 where T1","page":"Docstrings","title":"NumNN.layerBackProp","text":"layerBackProp(\n    cLayer::AddLayer,\n    model::Model,\n    FCache::Dict{Layer, Dict{Symbol, AbstractArray}},\n    BCache::Dict{Layer, Dict{Symbol, AbstractArray}},\n    dAo::AbstractArray{T1,N} = Array{Any,1}(undef,0);\n    labels::AbstractArray{T2,N} = Array{Any,1}(undef,0),\n    kwargs...,\n) where {T1,T2,N}\n\nPerform the back propagation of AddLayer type\n\nArgument\n\ncLayer := the layer to perform the backprop on\nmodel := the Model\nFCache := the cache values of the forprop\nBCache := the cache values of the backprop from the front Layer(s)\ndAo := (for test purpose) the derivative of the front Layer\nlabels := in case this is the output layer\n\nReturn\n\nA Dict{Symbol, AbstractArray}(:dA => dAi)\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.layerBackProp-Union{Tuple{N}, Tuple{T2}, Tuple{T1}, Tuple{Flatten,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{Flatten,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray{T1,N}}} where N where T2 where T1","page":"Docstrings","title":"NumNN.layerBackProp","text":"layerBackProp(\n    cLayer::Flatten,\n    model::Model,\n    FCache::Dict{Layer, Dict{Symbol, AbstractArray}},\n    BCache::Dict{Layer, Dict{Symbol, AbstractArray}},\n    dAo::AbstractArray{T1,N} = Array{Any,1}(undef,0);\n    labels::AbstractArray{T2,N} = Array{Any,1}(undef,0),\n    kwargs...,\n) where {T1,T2,N}\n\nPerform the back propagation of Flatten type\n\nArgument\n\ncLayer := the layer to perform the backprop on\nmodel := the Model\nFCache := the cache values of the forprop\nBCache := the cache values of the backprop from the front Layer(s)\ndAo := (for test purpose) the derivative of the front Layer\nlabels := in case this is the output layer\n\nReturn\n\nA Dict{Symbol, AbstractArray}(:dA => dAi)\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.layerBackProp-Union{Tuple{N}, Tuple{T2}, Tuple{T1}, Tuple{Input,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{Input,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray{T1,N}}} where N where T2 where T1","page":"Docstrings","title":"NumNN.layerBackProp","text":"layerBackProp(\n    cLayer::Input,\n    model::Model,\n    FCache::Dict{Layer, Dict{Symbol, AbstractArray}},\n    BCache::Dict{Layer, Dict{Symbol, AbstractArray}},\n    dAo::AbstractArray{T1,N} = Array{Any,1}(undef,0);\n    labels::AbstractArray{T2,N} = Array{Any,1}(undef,0),\n    kwargs...,\n) where {T1,T2,N}\n\nPerform the back propagation of Input type\n\nArgument\n\ncLayer := the layer to perform the backprop on\nmodel := the Model\nFCache := the cache values of the forprop\nBCache := the cache values of the backprop from the front Layer(s)\ndAo := (for test purpose) the derivative of the front Layer\nlabels := in case this is the output layer\n\nReturn\n\nA Dict{Symbol, AbstractArray}(:dA => dAi)\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.layerBackProp-Union{Tuple{PL}, Tuple{PL,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{PL,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray}, Tuple{PL,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray,AbstractArray}, Tuple{PL,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray,AbstractArray,AbstractArray}} where PL<:PoolLayer","page":"Docstrings","title":"NumNN.layerBackProp","text":"layerBackProp(     cLayer::PoolLayer,     model::Model,     FCache::Dict{Layer, Dict{Symbol, AbstractArray}},     BCache::Dict{Layer, Dict{Symbol, AbstractArray}},     Ai::AbstractArray = Array{Any,1}(undef,0),     Ao::AbstractArray = Array{Any,1}(undef,0),     dAo::AbstractArray = Array{Any,1}(undef,0);     labels::AbstractArray = Array{Any,1}(undef,0),     kwargs... )\n\nPerforms the layer back propagation for a PoolLayer\n\nArguments\n\ncLayer::ConvLayer\nmodel::Model\nFCache := the cache of the forward propagation step\nBCache := the cache of so far done back propagation\nfor test purpose   Ai   Ao   dAo\nlabels := when cLayer is an output Layer\n\nReturn\n\nDict(:dA => dAi)\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.layerBackProp-Union{Tuple{SoS}, Tuple{Activation,Model,SoS,AbstractArray,AbstractArray}} where SoS<:Union{Type{softmax}, Type{σ}}","page":"Docstrings","title":"NumNN.layerBackProp","text":"layerBackProp(\n    cLayer::Activation,\n    model::Model,\n    actFun::SoS,\n    Ao::AbstractArray,\n    labels::AbstractArray,\n) where {SoS<:Union{Type{softmax},Type{σ}}}\n\nFor output Activation layers with softmax and sigmoid activation functions\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.layerBackProp-Union{Tuple{SoS}, Tuple{ConvLayer,Model,SoS,AbstractArray,AbstractArray}} where SoS<:Union{Type{softmax}, Type{σ}}","page":"Docstrings","title":"NumNN.layerBackProp","text":"function layerBackProp(\n    cLayer::ConvLayer,\n    model::Model,\n    actFun::SoS,\n    Ao::AbstractArray,\n    labels::AbstractArray,\n) where {SoS<:Union{Type{softmax},Type{σ}}}\n\nDerive the loss function to the input of the activation function when activation is either softmax or σ\n\nReturn\n\ndZ::AbstractArray := the derivative of the loss function to the input of the activation function\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.layerBackProp-Union{Tuple{SoS}, Tuple{FCLayer,Model,SoS,AbstractArray,AbstractArray}} where SoS<:Union{Type{softmax}, Type{σ}}","page":"Docstrings","title":"NumNN.layerBackProp","text":"layerBackProp(\n    cLayer::FCLayer,\n    model::Model,\n    actFun::SoS,\n    Ao::AbstractArray,\n    labels::AbstractArray,\n) where {SoS<:Union{Type{softmax},Type{σ}}}\n\nFor output FCLayer layers with softmax and sigmoid activation functions\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.layerBackProp-Union{Tuple{T2}, Tuple{T1}, Tuple{FCLayer,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}}}, Tuple{FCLayer,Model,Dict{Layer,Dict{Symbol,AbstractArray}},Dict{Layer,Dict{Symbol,AbstractArray}},AbstractArray{T1,2}}} where T2 where T1","page":"Docstrings","title":"NumNN.layerBackProp","text":"layerBackProp(\n    cLayer::FCLayer,\n    model::Model,\n    FCache::Dict{Layer, Dict{Symbol, AbstractArray}},\n    BCache::Dict{Layer, Dict{Symbol, AbstractArray}},\n    dAo::AbstractArray{T1,2} = Array{Any,2}(undef,0,0);\n    labels::AbstractArray{T2,2} = Array{Any,2}(undef,0,0),\n    kwargs...,\n) where {T1,T2}\n\nPerform the back propagation of FCLayer type on the activations and trainable parameters W and B\n\nArgument\n\ncLayer := the layer to perform the backprop on\nmodel := the Model\nFCache := the cache values of the forprop\nBCache := the cache values of the backprop from the front Layer(s)\ndAo := (for test purpose) the derivative of the front Layer\nlabels := in case this is the output layer\n\nReturn\n\nA Dict{Symbol, AbstractArray}(:dA => dAi)\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.layerForProp","page":"Docstrings","title":"NumNN.layerForProp","text":"layerForProp(\n    cLayer::FCLayer,\n    Ai::AbstractArray = Array{Any,1}(undef,0);\n    FCache::Dict{Layer,Dict{Symbol, AbstractArray}},\n    kwargs...,\n)\n\nPerform forward propagation for FCLayer Layer\n\nArguments\n\ncLayer := the layer to perform for prop on\nAi := is the input activation of the FCLayer Layer\nFCache := a cache holder of the for prop\n\nReturn\n\nA Dict{Symbol, AbstractArray}(:A => Ao, :Z => Z)\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#NumNN.layerForProp","page":"Docstrings","title":"NumNN.layerForProp","text":"layerForProp(\n    cLayer::BatchNorm,\n    Ai::AbstractArray = Array{Any,1}(undef,0);\n    FCache::Dict{Layer,Dict{Symbol, AbstractArray}},\n    kwargs...,\n)\n\nPerform forward propagation for BatchNorm Layer and trainable parameters W and B\n\nArguments\n\ncLayer := the layer to perform for prop on\nAi := is the input activation of the BatchNorm Layer\nFCache := a cache holder of the for prop\n\nReturn\n\nDict(       :μ => μ,       :Ai_μ => Ai_μ,       :Ai_μ_s => Ai_μ_s,       :var => var,       :Z => Z,       :A => Ao,       :Ap => Ap,       )\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#NumNN.layerForProp","page":"Docstrings","title":"NumNN.layerForProp","text":"layerForProp(\n    cLayer::Activation,\n    Ai::AbstractArray = Array{Any,1}(undef,0);\n    FCache::Dict{Layer,Dict{Symbol, AbstractArray}},\n    kwargs...,\n)\n\nPerform forward propagation for Activation Layer\n\nArguments\n\ncLayer := the layer to perform for prop on\nAi := is the input activation of the Activation Layer\nFCache := a cache holder of the for prop\n\nReturn\n\nA Dict{Symbol, AbstractArray}(:A => Ao)\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#NumNN.layerForProp","page":"Docstrings","title":"NumNN.layerForProp","text":"layerForProp(\n    cLayer::Flatten,\n    Ai::AbstractArray = Array{Any,1}(undef,0);\n    FCache::Dict{Layer,Dict{Symbol, AbstractArray}},\n    kwargs...,\n)\n\nPerform forward propagation for Flatten Layer\n\nArguments\n\ncLayer := the layer to perform for prop on\nAi := is the input activation of the Flatten Layer\nFCache := a cache holder of the for prop\n\nReturn\n\nA Dict{Symbol, AbstractArray}(:A => Ao)\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#NumNN.layerForProp","page":"Docstrings","title":"NumNN.layerForProp","text":"layerForProp(\n    cLayer::Input,\n    X::AbstractArray = Array{Any,1}(undef,0);\n    FCache::Dict{Layer,Dict{Symbol, AbstractArray}},\n    kwargs...,\n)\n\nPerform forward propagation for Input Layer\n\nArguments\n\ncLayer := the layer to perform for prop on\nX := is the input data of the Input Layer\nFCache := a cache holder of the for prop\n\nReturn\n\nA Dict{Symbol, AbstractArray}(:A => Ao)\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#NumNN.layerForProp-Tuple{AddLayer}","page":"Docstrings","title":"NumNN.layerForProp","text":"layerForProp(\n    cLayer::AddLayer;\n    FCache::Dict{Layer,Dict{Symbol, AbstractArray}},\n    kwargs...,\n)\n\nPerform forward propagation for AddLayer Layer\n\nArguments\n\ncLayer := the layer to perform for prop on\nFCache := a cache holder of the for prop\n\nReturn\n\nA Dict{Symbol, AbstractArray}(:A => Ao)\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.layerForProp-Union{Tuple{CL}, Tuple{CL}, Tuple{CL,AbstractArray}} where CL<:ConvLayer","page":"Docstrings","title":"NumNN.layerForProp","text":"function layerForProp(\n    cLayer::ConvLayer,\n    Ai::AbstractArray = Array{Any,1}(undef,0);\n    FCache::Dict{Layer,Dict{Symbol, AbstractArray}},\n    kwargs...\n)\n\nPerform the layer forward propagation for a ConvLayer\n\nArguments\n\ncLayer::ConvLayer\nAi := optional activation of the previous layer\nFCache := a Dict holds the outputs of layerForProp of the previous Layer(s)\n\nReturns\n\nDict(:Z => Z, :A => Ao)\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.layerForProp-Union{Tuple{PL}, Tuple{PL}, Tuple{PL,AbstractArray}} where PL<:PoolLayer","page":"Docstrings","title":"NumNN.layerForProp","text":"layerForProp(\n    cLayer::PoolLayer},\n    Ai::AbstractArray = Array{Any,1}(undef,0);\n    FCache::Dict{Layer,Dict{Symbol, AbstractArray}},\n    kwargs...\n)\n\nPerform the layer forward propagation for a PoolLayer\n\nArguments\n\ncLayer::PoolLayer\nAi := optional activation of the previous layer\nFCache := a Dict holds the outputs of layerForProp of the previous Layer(s)\n\nReturns\n\nDict(:A => Ao)\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.layerUpdateParams!-Union{Tuple{FoB}, Tuple{Model,FoB}, Tuple{Model,FoB,Integer}} where FoB<:Union{BatchNorm, FCLayer}","page":"Docstrings","title":"NumNN.layerUpdateParams!","text":"function layerUpdateParams!(\n    model::Model,\n    cLayer::FoB,\n    cnt::Integer = -1;\n    tMiniBatch::Integer = 1,\n    kwargs...,\n) where {FoB <: Union{FCLayer, BatchNorm}}\n\nupdate trainable parameters for FCLayer and BatchNorm layers\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.oneHot-Tuple{Any}","page":"Docstrings","title":"NumNN.oneHot","text":"oneHot(Y; classes = [], numC = 0)\n\nconvert array of integer classes into one Hot coding.\n\nArguments\n\nY := a vector of classes as a number\nclasses := the classes explicity represented (in case not all the classes are present in the labels given)\nnumC := number of classes as alternative to classes variable\n\nExamples\n\n```julia Y = rand(0:9, 100); # a 100 item with class of [0-9]\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.padding-Union{Tuple{T}, Tuple{AbstractArray{T,4},Integer,Integer,Integer,Integer}} where T","page":"Docstrings","title":"NumNN.padding","text":"function padding(Ai::AbstractArray{T,4},\n                 p_H::Integer,\n                 p_W::Integer=-1) where {T}\n\npad zeros to the Array Ai with amount of p values\n\ninputs:     Ai := Array of type T and dimension N     p  := integer determinde the amount of zeros padding           i.e.           if Ai is a 3-dimensional array the padding will be for the first               dimension           if Ai is a 4-dimensional array the padding will be for the first 2               dimensions           if Ai is a 5-dimensional array the padding will be for the first 3               dimensions\n\noutput:     PaddinView array where it contains the padded values and the original         data without copying it\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.paddingSize-Union{Tuple{PL}, Tuple{PL,Tuple}} where PL<:NumNN.PaddableLayer","page":"Docstrings","title":"NumNN.paddingSize","text":"function paddingSize(cLayer::PL, Ai::AbstractArray) where {PL<:PaddableLayer}\n\nHelping function that returns the pHhi, pHlo, and (in case 2D Conv), pWhi, pWlo, and so on\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.predict","page":"Docstrings","title":"NumNN.predict","text":"predict(model::Model, X_In::AbstractArray, Y_In = nothing; kwargs...)\n\nRun the prediction based on the trained model\n\nArguments\n\nmodel::Model := the trained Model to predict on\nX_In := the input Array\nY_In := labels (optional) to evaluate the model\n\nKey-word Arugmets\n\nbatchSize := default 32\nuseProgBar := (Bool) where or not to shoe the prograss bar\n\nReturn\n\na Dict of:\n:YhatValue := Array of the output of the integer prediction values\n:YhatProb := Array of the output probabilities\n:accuracy := the accuracy of prediction in case Y_In is given\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#NumNN.predictBatch","page":"Docstrings","title":"NumNN.predictBatch","text":"predictBatch(model::Model, X::AbstractArray, Y = nothing; kwargs...)\n\npredict Y using the model and the input X and the labels Y\n\nInputs\n\nmodel::Model := the trained model\nX::AbstractArray := the input Array\nY := the input labels to compare with (optional)\n\nOutput\n\na Tuple of\nŶ := the predicted values\nŶ_bool := the predicted labels\n\"accuracy\" := the accuracy of the predicted labels\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#NumNN.prevnextfloat-Tuple{Any}","page":"Docstrings","title":"NumNN.prevnextfloat","text":"return previous float if x == 1 and nextfloat if x == 0\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.probToValue-Union{Tuple{N}, Tuple{T}, Tuple{Aa}, Tuple{Type{σ},AbstractArray{T,N}}, Tuple{Type{σ},AbstractArray{T,N},Aa}} where N where T where Aa<:(Union{Nothing, #s27} where #s27<:AbstractArray)","page":"Docstrings","title":"NumNN.probToValue","text":"function probToValue(\n    actFun::Type{σ},\n    probs::AbstractArray{T,N},\n    labels::Aa = nothing;\n    evalConst = 0.5,\n) where {Aa<:Union{<:AbstractArray,Nothing},T,N}\n\nConvert the probabilities return out of sigmoid function to Bool value (i.e. 0,1) values based on comparing on a threshold value evalConst\n\nReturn\n\nŶ_bool := Boolean valuse of the probabilites\nacc := Accuracy when labels provided\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.probToValue-Union{Tuple{S}, Tuple{N}, Tuple{T}, Tuple{Type{S},AbstractArray{T,N}}} where S<:NumNN.softmaxFamily where N where T","page":"Docstrings","title":"NumNN.probToValue","text":"function probToValue(\n    actFun::Type{S},\n    probs::AbstractArray{T,N};\n    labels = nothing,\n) where {T,N,S<:softmaxFamily}\n\nconvert the probabilites out of softmax or softmax-like functions into Bool values, where the max value gets 1 and the other get zeros\n\nReturn\n\nŶ_bool := Boolean valuse of the probabilites\nacc := Accuracy when labels provided\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.resetCount!-Tuple{Layer,Symbol}","page":"Docstrings","title":"NumNN.resetCount!","text":"resetCount!(outLayer::Layer, cnt::Symbol)\n\nto reset a counter in all layers under outLayer.\n\nArguments\n\noutLayer::Layer := the layer from start reseting the counter\ncnt::Symbol := the counter to be reseted\n\nExamples\n\nX_train = rand(128, 100);\n\nX_Input = Input(X_train);\nX = FCLayer(50, :relu)(X_Input);\nX_out = FCLayer(10, :softmax)(X);\n\nFCache = chainForProp(X_train, X_Input);\n\n# Now to reset the forwCount in all layers\n\nresetCount!(X_out, :forwCount)\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.train-Tuple{Any,Any,Model,Any}","page":"Docstrings","title":"NumNN.train","text":"train(\n      X_train,\n      Y_train,\n      model::Model,\n      epochs;\n      testData = nothing,\n      testLabels = nothing,\n      kwargs...,\n      )\n\nRepeat the trainging (forward/backward propagation and update parameters)\n\nArgument\n\nX_train := the training data\nY_train := the training labels\nmodel   := the model to train\nepochs  := the number of repetitions of the training phase\n\nKey-word Arguments\n\ntestData := to evaluate the training process over test data too\ntestLabels := to evaluate the training process over test data too\nbatchSize := the size of training when mini batch training\n\nuseProgBar` := (true, false) value to use prograss bar\n\nkwargs := other key-word Arguments to pass for the lower functions in hierarchy\n\nReturn\n\nA Dict{Symbol, Vector} of:\n:trainAccuracies := an Array of the accuracies of training data at each epoch\n:trainCosts := an Array of the costs of training data at each epoch\nIn case testDate and testLabels are givens:\n:testAccuracies := an Array of the accuracies of test data at each epoch\n:testCosts := an Array of the costs of test data at each epoch\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.unroll","page":"Docstrings","title":"NumNN.unroll","text":"unroll(cLayer::Conv3D, AiS::Tuple, param::Symbol=:W)\n\nunroll the param of Conv3D into 2D matrix\n\nArguments\n\ncLayer := the layer of the paramters to unroll\nAiS := the padded input to determinde the size and shape of the output of unroll\nparam := Conv1D parameter to be unrolled\n\nReturn\n\nK := 2D Matrix of the param\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#NumNN.unroll","page":"Docstrings","title":"NumNN.unroll","text":"unroll(cLayer::Conv2D, AiS::Tuple, param::Symbol=:W)\n\nunroll the param of Conv1D into 2D matrix\n\nArguments\n\ncLayer := the layer of the paramters to unroll\nAiS := the padded input to determinde the size and shape of the output of unroll\nparam := Conv1D parameter to be unrolled\n\nReturn\n\nK := 2D Matrix of the param\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#NumNN.unroll","page":"Docstrings","title":"NumNN.unroll","text":"unroll(cLayer::Conv1D, AiS::Tuple, param::Symbol=:W)\n\nunroll the param of Conv1D into 2D matrix\n\nArguments\n\ncLayer := the layer of the paramters to unroll\nAiS := the padded input to determinde the size and shape of the output of unroll\nparam := Conv1D parameter to be unrolled\n\nReturn\n\nK := 2D Matrix of the param\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#NumNN.PaddableLayer","page":"Docstrings","title":"NumNN.PaddableLayer","text":"Summary\n\nabstract type PaddableLayer <: Layer\n\nAbstract Type to hold all Paddable Layers (i.e.  ConvLayer & PoolLayer)\n\nSubtypes\n\nConvLayer\nPoolLayer\n\nSupertype Hierarchy\n\nPaddableLayer <: Layer <: Any\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#Base.getindex-Tuple{Any,Any}","page":"Docstrings","title":"Base.getindex","text":"getindex(it, key; default) = haskey(it, key) ? it[key] : default\n\nExamples\n\nD = Dict(:A=>\"A\", :B=>\"B\")\n\nA = getindex(D, :A)\n\n## this will return an error\n#C = getindex(D: :C)\n\n#instead\nC = getindex(D, :C; default=\"C\")\n#this will return the `String` C\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.cost-Union{Tuple{N}, Tuple{T2}, Tuple{T1}, Tuple{Type{binaryCrossentropy},AbstractArray{T1,N},AbstractArray{T2,N}}} where N where T2 where T1","page":"Docstrings","title":"NumNN.cost","text":"function cost(\n    loss::Type{binaryCrossentropy},\n    A::AbstractArray{T1,N},\n    Y::AbstractArray{T2,N},\n) where {T1, T2, N}\n\nCompute the cost for binaryCrossentropy loss function\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#NumNN.cost-Union{Tuple{N}, Tuple{T2}, Tuple{T1}, Tuple{Type{categoricalCrossentropy},AbstractArray{T1,N},AbstractArray{T2,N}}} where N where T2 where T1","page":"Docstrings","title":"NumNN.cost","text":"function cost(\n    loss::Type{categoricalCrossentropy},\n    A::AbstractArray{T1,N},\n    Y::AbstractArray{T2,N},\n) where {T1, T2, N}\n\nCompute the cost for categoricalCrossentropy loss function\n\n\n\n\n\n","category":"method"}]
}
