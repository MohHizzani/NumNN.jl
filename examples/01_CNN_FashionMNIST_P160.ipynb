{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Tutorial on Fashion MNIST Data Set\n",
    "\n",
    "\n",
    "This turorial gives a breif intro on using CNN for train and prediction (i.e. inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Load Some Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Plots.GRBackend()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLDatasets\n",
    "using NumNN\n",
    "using Plots\n",
    "gr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temp for ProgressMeter.jl Package\n",
    "\n",
    "**Uncomment the following line if you run this code for the first time***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ] add https://github.com/timholy/ProgressMeter.jl.git ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ProgressMeter\n",
    "ProgressMeter.ijulia_behavior(:clear);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Train/Test Data/Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = FashionMNIST.traindata(Float64);\n",
    "X_test, Y_test = FashionMNIST.testdata(Float64);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Prepare the data/labels\n",
    "\n",
    "Since the shape of the MNIST data is `(28,28,size)` and to use it in CNN 2D it must be as 4D Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SigmoidNumbers\n",
    "P1 = Posit{16,0}\n",
    "P2 = Posit{16,1}\n",
    "\n",
    "Base.convert(::Type{P}, b::Bool) where {P <: Posit} = P.(Float64(b))\n",
    "Posit{N,ES}(b::Bool) where {N,ES} = convert(Posit{N,ES}, b)\n",
    "Base.exp(a::P) where {P <: Posit} = P.(exp(Float64(a)))\n",
    "Base.promote_rule(::Type{P}, ::Type{IE}) where {P <: Posit, IE <: Base.IEEEFloat} = P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = P1.(reshape(X_train, (size(X_train)[1:2]..., 1, size(X_train)[end])))\n",
    "X_test = P1.(reshape(X_test, (size(X_test)[1:2]...,1,size(X_test)[end])))\n",
    "\n",
    "Y_train = oneHot(Y_train)\n",
    "Y_test = oneHot(Y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Input = Input(X_train)\n",
    "X = Conv2D(10, (3,3))(X_Input)\n",
    "X = BatchNorm(dim=3)(X) #to normalize across the channels\n",
    "X = Activation(:relu)(X)\n",
    "X = MaxPool2D((2,2))(X)\n",
    "X = Conv2D(20, (5,5))(X)\n",
    "X = BatchNorm(dim=3)(X)\n",
    "X = Activation(:relu)(X)\n",
    "X = AveragePool2D((3,3))(X)\n",
    "X = Flatten()(X)\n",
    "X_Output = FCLayer(10, :softmax)(X);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way when there is no side branches is to use the `chain` function as follows:\n",
    "\n",
    "```julia\n",
    "X_Input, X_Ouput = chain(X_train,[Conv2D(10, (3,3)),\n",
    "                                  BatchNorm(dim=3),\n",
    "                                  Activation(:relu),\n",
    "                                  MaxPool2D((2,2)),\n",
    "                                  Conv2D(20, (5,5)),\n",
    "                                  BatchNorm(dim=3),\n",
    "                                  Activation(:relu),\n",
    "                                  AveragePool2D((3,3)),\n",
    "                                  Flatten(),\n",
    "                                  FCLayer(10,:softmax)]);\n",
    "```\n",
    "\n",
    "`chain` returns a `Tuple` of two pointers of the Input `Layer` and Output `Layer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Define Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will also initialize the `Layer`s' parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(X_train,Y_train,X_Input,X_Output, 0.005; optimizer=:adam, paramsDtype=P1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use `predict` to see the current Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  31%|████████████▉                            |  ETA: 0:02:03\u001b[39m\n",
      "\u001b[34m  Instances 10000:  3136\u001b[39m"
     ]
    }
   ],
   "source": [
    "TestP = predict(model, X_test, Y_test);\n",
    "\n",
    "println()\n",
    "println(\"The accuracy of Test Data before the training process $(round(TestP[:accuracy], digits=4))\")\n",
    "println(\"The cost of Test Data before the training process $(round(TestP[:cost], digits=4))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:18:42\u001b[39m\n",
      "\u001b[34m  Instances 60000:  60000\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of Train Data before the training process 0.1027\n"
     ]
    }
   ],
   "source": [
    "TrainP = predict(model, X_train, Y_train);\n",
    "\n",
    "println()\n",
    "println(\"The accuracy of Train Data before the training process $(round(TrainP[:accuracy], digits=4))\")\n",
    "println(\"The cost of Train Data before the training process $(round(TrainP[:cost], digits=4))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SigmoidNumbers.NaNError",
     "evalue": "SigmoidNumbers.NaNError(/, Any[Posit{16,0}(0x0000), Posit{16,0}(0x0000)])",
     "output_type": "error",
     "traceback": [
      "SigmoidNumbers.NaNError(/, Any[Posit{16,0}(0x0000), Posit{16,0}(0x0000)])",
      "",
      "Stacktrace:",
      " [1] macro expansion at /home/mhizzani/.julia/packages/SigmoidNumbers/JPzKW/src/Math/muldiv.jl:342 [inlined]",
      " [2] /(::Posit{16,0}, ::Posit{16,0}) at /home/mhizzani/.julia/packages/SigmoidNumbers/JPzKW/src/Math/muldiv.jl:275",
      " [3] _broadcast_getindex_evalf at ./broadcast.jl:631 [inlined]",
      " [4] _broadcast_getindex at ./broadcast.jl:604 [inlined]",
      " [5] getindex at ./broadcast.jl:564 [inlined]",
      " [6] macro expansion at ./broadcast.jl:910 [inlined]",
      " [7] macro expansion at ./simdloop.jl:77 [inlined]",
      " [8] copyto! at ./broadcast.jl:909 [inlined]",
      " [9] copyto! at ./broadcast.jl:864 [inlined]",
      " [10] copy at ./broadcast.jl:840 [inlined]",
      " [11] materialize at ./broadcast.jl:820 [inlined]",
      " [12] softmax(::Array{Posit{16,0},2}) at /home/mhizzani/Codes/NumNN/src/actFuns.jl:104",
      " [13] top-level scope at /home/mhizzani/.julia/packages/IJulia/DrVMH/src/kernel.jl:52",
      " [14] eval at ./boot.jl:331 [inlined]",
      " [15] eval at /home/mhizzani/Codes/NumNN/src/NumNN.jl:1 [inlined]",
      " [16] layerForProp(::FCLayer, ::Array{Any,1}; FCache::Dict{Layer,Dict{Symbol,AbstractArray}}, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:batchSize,),Tuple{Int64}}}) at /home/mhizzani/Codes/NumNN/src/parallelLayerForProp.jl:83",
      " [17] chainForProp(::Array{Posit{16,0},4}, ::FCLayer, ::Int64; FCache::Dict{Layer,Dict{Symbol,AbstractArray}}, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:batchSize,),Tuple{Int64}}}) at /home/mhizzani/Codes/NumNN/src/parallelBackForProp.jl:52",
      " [18] chainForProp(::Array{Posit{16,0},4}, ::Flatten, ::Int64; FCache::Dict{Layer,Dict{Symbol,AbstractArray}}, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:batchSize,),Tuple{Int64}}}) at /home/mhizzani/Codes/NumNN/src/parallelBackForProp.jl:78 (repeats 10 times)",
      " [19] train(::Array{Posit{16,0},4}, ::BitArray{2}, ::Model, ::Int64; testData::Nothing, testLabels::Nothing, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:batchSize,),Tuple{Int64}}}) at /home/mhizzani/Codes/NumNN/src/parallelBackForProp.jl:605",
      " [20] top-level scope at In[17]:1"
     ]
    }
   ],
   "source": [
    "TrainD = train(X_train, Y_train, model, 10; batchSize=16);#; testData = X_test, testLabels = Y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train` function provides an extra `kwargs` to use test Data/Labels to get the Costs and Accuracies during each training epoch. \n",
    "\n",
    "**Note** This will take extra time to do the training\n",
    "\n",
    "Instead it can be used as follows:\n",
    "\n",
    "```julia\n",
    "TrainD = train(X_train, Y_train, model, 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: TrainD not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: TrainD not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[12]:1"
     ]
    }
   ],
   "source": [
    "plot(1:10, TrainD[:trainAccuracies], label=\"Training Accuracies\")\n",
    "plot!(1:10, TrainD[:trainCosts], label=\"Training Costs\")\n",
    "plot!(1:10, TrainD[:testAccuracies], label=\"Test Accuracies\")\n",
    "plot!(1:10, TrainD[:testCosts], label=\"Test Costs\")\n",
    "ylabel!(\"Epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  30%|████████████▏                            |  ETA: 0:17:45\u001b[39m\n",
      "\u001b[34m  Instances 60000:  17728\u001b[39m"
     ]
    },
    {
     "ename": "InterruptException",
     "evalue": "SYSTEM: show(lasterr) caused an error",
     "output_type": "error",
     "traceback": [
      "SYSTEM: show(lasterr) caused an error",
      "",
      "Stacktrace:",
      " [1] NNConv(::Conv2D, ::Array{Posit{16,0},4}) at /home/mhizzani/Codes/NumNN/src/cnn/parallelNNConv.jl:20",
      " [2] layerForProp(::Conv2D, ::Array{Any,1}; FCache::Dict{Layer,Dict{Symbol,AbstractArray}}, kwargs::Base.Iterators.Pairs{Symbol,Bool,Tuple{Symbol},NamedTuple{(:prediction,),Tuple{Bool}}}) at /home/mhizzani/Codes/NumNN/src/cnn/parallelLayerForProp.jl:51",
      " [3] chainForProp(::SubArray{Posit{16,0},4,Array{Posit{16,0},4},Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64},UnitRange{Int64}},false}, ::Conv2D, ::Int64; FCache::Dict{Layer,Dict{Symbol,AbstractArray}}, kwargs::Base.Iterators.Pairs{Symbol,Bool,Tuple{Symbol},NamedTuple{(:prediction,),Tuple{Bool}}}) at /home/mhizzani/Codes/NumNN/src/parallelBackForProp.jl:74",
      " [4] chainForProp(::SubArray{Posit{16,0},4,Array{Posit{16,0},4},Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64},UnitRange{Int64}},false}, ::MaxPool2D, ::Int64; FCache::Dict{Layer,Dict{Symbol,AbstractArray}}, kwargs::Base.Iterators.Pairs{Symbol,Bool,Tuple{Symbol},NamedTuple{(:prediction,),Tuple{Bool}}}) at /home/mhizzani/Codes/NumNN/src/parallelBackForProp.jl:78 (repeats 5 times)",
      " [5] predictBatch(::Model, ::SubArray{Posit{16,0},4,Array{Posit{16,0},4},Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64},UnitRange{Int64}},false}, ::SubArray{Bool,2,BitArray{2},Tuple{Base.OneTo{Int64},UnitRange{Int64}},false}; kwargs::Base.Iterators.Pairs{Symbol,Bool,Tuple{Symbol},NamedTuple{(:prediction,),Tuple{Bool}}}) at /home/mhizzani/Codes/NumNN/src/parallelBackForProp.jl:121",
      " [6] macro expansion at /home/mhizzani/Codes/NumNN/src/parallelBackForProp.jl:205 [inlined]",
      " [7] macro expansion at ./simdloop.jl:77 [inlined]",
      " [8] predict(::Model, ::Array{Posit{16,0},4}, ::BitArray{2}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/mhizzani/Codes/NumNN/src/parallelBackForProp.jl:198",
      " [9] predict(::Model, ::Array{Posit{16,0},4}, ::BitArray{2}) at /home/mhizzani/Codes/NumNN/src/parallelBackForProp.jl:164",
      " [10] top-level scope at In[13]:1"
     ]
    }
   ],
   "source": [
    "TrainP = predict(model, X_train, Y_train);\n",
    "\n",
    "println()\n",
    "println(\"The accuracy of Train Data before the training process $(round(TrainP[:accuracy], digits=4))\")\n",
    "println(\"The cost of Train Data before the training process $(round(TrainP[:cost], digits=4))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "InterruptException",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] NNConv(::Conv2D, ::Array{Posit{16,0},4}) at /home/mhizzani/Codes/NumNN/src/cnn/parallelNNConv.jl:20",
      " [2] layerForProp(::Conv2D, ::Array{Any,1}; FCache::Dict{Layer,Dict{Symbol,AbstractArray}}, kwargs::Base.Iterators.Pairs{Symbol,Bool,Tuple{Symbol},NamedTuple{(:prediction,),Tuple{Bool}}}) at /home/mhizzani/Codes/NumNN/src/cnn/parallelLayerForProp.jl:51",
      " [3] chainForProp(::SubArray{Posit{16,0},4,Array{Posit{16,0},4},Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64},UnitRange{Int64}},false}, ::Conv2D, ::Int64; FCache::Dict{Layer,Dict{Symbol,AbstractArray}}, kwargs::Base.Iterators.Pairs{Symbol,Bool,Tuple{Symbol},NamedTuple{(:prediction,),Tuple{Bool}}}) at /home/mhizzani/Codes/NumNN/src/parallelBackForProp.jl:74",
      " [4] chainForProp(::SubArray{Posit{16,0},4,Array{Posit{16,0},4},Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64},UnitRange{Int64}},false}, ::MaxPool2D, ::Int64; FCache::Dict{Layer,Dict{Symbol,AbstractArray}}, kwargs::Base.Iterators.Pairs{Symbol,Bool,Tuple{Symbol},NamedTuple{(:prediction,),Tuple{Bool}}}) at /home/mhizzani/Codes/NumNN/src/parallelBackForProp.jl:78 (repeats 5 times)",
      " [5] predictBatch(::Model, ::SubArray{Posit{16,0},4,Array{Posit{16,0},4},Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64},UnitRange{Int64}},false}, ::SubArray{Bool,2,BitArray{2},Tuple{Base.OneTo{Int64},UnitRange{Int64}},false}; kwargs::Base.Iterators.Pairs{Symbol,Bool,Tuple{Symbol},NamedTuple{(:prediction,),Tuple{Bool}}}) at /home/mhizzani/Codes/NumNN/src/parallelBackForProp.jl:121",
      " [6] macro expansion at /home/mhizzani/Codes/NumNN/src/parallelBackForProp.jl:205 [inlined]",
      " [7] macro expansion at ./simdloop.jl:77 [inlined]",
      " [8] predict(::Model, ::Array{Posit{16,0},4}, ::BitArray{2}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/mhizzani/Codes/NumNN/src/parallelBackForProp.jl:198",
      " [9] predict(::Model, ::Array{Posit{16,0},4}, ::BitArray{2}) at /home/mhizzani/Codes/NumNN/src/parallelBackForProp.jl:164",
      " [10] top-level scope at In[14]:1"
     ]
    }
   ],
   "source": [
    "TestP = predict(model, X_test, Y_test);\n",
    "\n",
    "println()\n",
    "println(\"The accuracy of Test Data before the training process $(round(TestP[:accuracy], digits=4))\")\n",
    "println(\"The cost of Test Data before the training process $(round(TestP[:cost], digits=4))\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
